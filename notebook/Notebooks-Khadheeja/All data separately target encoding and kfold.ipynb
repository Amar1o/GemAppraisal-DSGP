{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oEirAFIwRzVffJf_ns7taUb5H8nwPLFl","timestamp":1733727958649},{"file_id":"1MxQxwDg4UPch1hT6i9lKld83Kko3DUpX","timestamp":1733717296564}],"authorship_tag":"ABX9TyOqilrXMWRB7yZ9NC8MMbbp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"pCCa9eg_YB3T"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"c3_5mH9t741K","executionInfo":{"status":"ok","timestamp":1733887729274,"user_tz":-330,"elapsed":3540,"user":{"displayName":"Khadheeja","userId":"04772930763469062553"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","#to ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["Upload data"],"metadata":{"id":"N9OX-xYBYF6b"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"9UpROjtRXkOS","executionInfo":{"status":"ok","timestamp":1733887766728,"user_tz":-330,"elapsed":37460,"user":{"displayName":"Khadheeja","userId":"04772930763469062553"}},"outputId":"41293298-9369-4f10-9311-aaadcfa4abb9"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-cf7122c0-e4f0-4380-8652-b2bdef5fadd2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-cf7122c0-e4f0-4380-8652-b2bdef5fadd2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving blue_data.csv to blue_data.csv\n","Saving green_data.csv to green_data.csv\n","Saving padparadscha_data.csv to padparadscha_data.csv\n","Saving pink_data.csv to pink_data.csv\n","Saving purple_data.csv to purple_data.csv\n","Saving ruby_data.csv to ruby_data.csv\n","Saving white_data.csv to white_data.csv\n","Saving yellow_data.csv to yellow_data.csv\n"]}]},{"cell_type":"markdown","source":["Read csv"],"metadata":{"id":"VIyfRZNCYKb0"}},{"cell_type":"code","source":["blue_data = pd.read_csv(\"blue_data.csv\")\n","green_data = pd.read_csv(\"green_data.csv\")\n","padparadscha_data = pd.read_csv(\"padparadscha_data.csv\")\n","pink_data = pd.read_csv(\"pink_data.csv\")\n","purple_data = pd.read_csv(\"purple_data.csv\")\n","ruby_data = pd.read_csv(\"ruby_data.csv\")\n","white_data = pd.read_csv(\"white_data.csv\")\n","yellow_data = pd.read_csv(\"yellow_data.csv\")"],"metadata":{"id":"blOBnSQaXwKq","executionInfo":{"status":"ok","timestamp":1733887766728,"user_tz":-330,"elapsed":14,"user":{"displayName":"Khadheeja","userId":"04772930763469062553"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["data = [blue_data, green_data, padparadscha_data, pink_data, purple_data, ruby_data, white_data, yellow_data]\n","data_names = [\"blue_data\", \"green_data\", \"padparadscha_data\", \"pink_data\", \"purple_data\", \"ruby_data\", \"white_data\", \"yellow_data\"]"],"metadata":{"id":"oAa4VhYA_DpG","executionInfo":{"status":"ok","timestamp":1733887766729,"user_tz":-330,"elapsed":14,"user":{"displayName":"Khadheeja","userId":"04772930763469062553"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Feature Engineering"],"metadata":{"id":"a15zCRqrQar1"}},{"cell_type":"code","source":["# STRATIFIES K-FOLD CROSS VALIDATION\n","\n","# Import Required Modules.\n","from statistics import mean, stdev\n","from sklearn import preprocessing\n","from sklearn.model_selection import KFold\n","from sklearn.ensemble import RandomForestRegressor\n","from xgboost import XGBRegressor\n","import xgboost\n","from sklearn import datasets\n","from sklearn.metrics import r2_score\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","count = 0\n","for dat in data:\n","    y_data=dat['total_price']\n","    X_data=dat.drop(['total_price'],axis=1)\n","\n","\n","    # Create KFold object.\n","    kf = KFold(n_splits=10, shuffle=True, random_state=36)\n","    r2_test = []\n","    r2_train = []\n","\n","    for train_index, test_index in kf.split(X_data, y_data):\n","        # Use `.iloc` to properly select rows by position\n","        X_train_fold = X_data.iloc[train_index].copy()\n","        X_test_fold = X_data.iloc[test_index].copy()\n","        y_train_fold = y_data.iloc[train_index].copy()\n","        y_test_fold = y_data.iloc[test_index].copy()\n","\n","        # Handle ordinal encoding\n","        #oe = OrdinalEncoder(categories=[['Poor', 'Fair', 'Good','Very Good','Excellent']])\n","        #X_train_fold['cut_quality_encoded'] = oe.fit_transform(X_train_fold[['cut_quality']])\n","        #X_test_fold['cut_quality_encoded'] = oe.fit_transform(X_test_fold[['cut_quality']])\n","\n","\n","\n","        # Handle target\n","        shape_means = X_train_fold.groupby('shape')['price_per_carat'].mean()\n","        origin_means = X_train_fold.groupby('origin')['price_per_carat'].mean()\n","        color_means = X_train_fold.groupby('color')['price_per_carat'].mean()\n","        color_intensity_means = X_train_fold.groupby('color_intensity')['price_per_carat'].mean()\n","        clarity_means = X_train_fold.groupby('clarity')['price_per_carat'].mean()\n","        treatment_means = X_train_fold.groupby('treatment')['price_per_carat'].mean()\n","        cut_means = X_train_fold.groupby('cut')['price_per_carat'].mean()\n","        cut_quality_means = X_train_fold.groupby('cut_quality')['price_per_carat'].mean()\n","\n","        # Map target encoding to the training data\n","        X_train_fold['shape_encoded'] = X_train_fold['shape'].map(shape_means)\n","        X_train_fold['origin_encoded'] = X_train_fold['origin'].map(origin_means)\n","        X_train_fold['color_encoded'] = X_train_fold['color'].map(color_means)\n","        X_train_fold['color_intensity_encoded'] = X_train_fold['color_intensity'].map(color_intensity_means)\n","        X_train_fold['clarity_encoded'] = X_train_fold['clarity'].map(clarity_means)\n","        X_train_fold['cut_encoded'] = X_train_fold['cut'].map(cut_means)\n","        X_train_fold['treatment_encoded'] = X_train_fold['treatment'].map(treatment_means)\n","        X_train_fold['cut_quality_encoded'] = X_train_fold['cut_quality'].map(cut_quality_means)\n","\n","        # Map target encoding to the test data\n","        # Handle unseen categories by filling with global mean\n","        global_mean = X_train_fold['price_per_carat'].mean()\n","        X_test_fold['shape_encoded'] = X_test_fold['shape'].map(shape_means).fillna(global_mean)\n","        X_test_fold['origin_encoded'] = X_test_fold['origin'].map(origin_means).fillna(global_mean)\n","        X_test_fold['color_encoded'] = X_test_fold['color'].map(color_means).fillna(global_mean)\n","        X_test_fold['color_intensity_encoded'] = X_test_fold['color_intensity'].map(color_intensity_means).fillna(global_mean)\n","        X_test_fold['clarity_encoded'] = X_test_fold['clarity'].map(clarity_means).fillna(global_mean)\n","        X_test_fold['cut_encoded'] = X_test_fold['cut'].map(cut_means).fillna(global_mean)\n","        X_test_fold['treatment_encoded'] = X_test_fold['treatment'].map(treatment_means).fillna(global_mean)\n","        X_test_fold['cut_quality_encoded'] = X_test_fold['cut_quality'].map(cut_quality_means).fillna(global_mean)\n","\n","\n","\n","        X_train_fold = X_train_fold.drop(columns=['color', 'shape', 'color_intensity', 'origin', 'cut', 'treatment', 'clarity','price_per_carat','cut_quality'])\n","        X_test_fold = X_test_fold.drop(columns=['color', 'shape', 'color_intensity', 'origin', 'cut', 'treatment', 'clarity','price_per_carat','cut_quality'])\n","        #print(X_train_fold.dtypes)\n","\n","        # Train the model.\n","        rfr = RandomForestRegressor(random_state=42)\n","        rfr.fit(X_train_fold, y_train_fold)\n","\n","        # Predictions\n","        y_pred_fold=rfr.predict(X_test_fold)\n","\n","        # Evaluate the model\n","        r2 = r2_score(y_test_fold,y_pred_fold)\n","        r2_test.append(r2)\n","\n","        # Check overfitting\n","        y_pred_train_fold=rfr.predict(X_train_fold)\n","\n","        train_r2=r2_score(y_train_fold, y_pred_train_fold)\n","        r2_train.append(train_r2)\n","\n","\n","\n","    # Print the output.\n","    print(data_names[count])\n","    print(\"\\n\")\n","    print('List of possible accuracy for test:', r2_test)\n","    print('\\nMaximum Testing Accuracy That can be obtained from this model is:',\n","      max(r2_test)*100, '%')\n","    print('\\nMinimum Testing Accuracy:',\n","      min(r2_test)*100, '%')\n","    print('\\nOverall Testing Accuracy:',\n","      mean(r2_test)*100, '%')\n","    print('\\nStandard Deviation is:', stdev(r2_test))\n","\n","    # Print the output.\n","    print('List of possible accuracy for train:', r2_train)\n","    print('\\nMaximum Training Accuracy That can be obtained from this model is:',\n","      max(r2_train)*100, '%')\n","    print('\\nMinimum Training Training Accuracy:',\n","      min(r2_train)*100, '%')\n","    print('\\nOverall Training Accuracy:',\n","      mean(r2_train)*100, '%')\n","    print('\\nStandard Deviation is:', stdev(r2_train))\n","    count +=1\n","    print(\"---------------------------------------------------------------------\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_936zGjrdk5P","executionInfo":{"status":"ok","timestamp":1733888402506,"user_tz":-330,"elapsed":90730,"user":{"displayName":"Khadheeja","userId":"04772930763469062553"}},"outputId":"353a2f14-e34f-4900-d456-5f04ddb7b964"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["blue_data\n","\n","\n","List of possible accuracy for test: [0.21143462247369815, 0.7424777659965378, 0.6304066794391217, 0.7451414959704972, 0.9356399941003261, 0.7970551324677831, 0.8090125152143062, 0.8874978505848786, 0.4536746794431211, 0.898220378366545]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 93.56399941003261 %\n","\n","Minimum Testing Accuracy: 21.143462247369815 %\n","\n","Overall Testing Accuracy: 71.10561114056814 %\n","\n","Standard Deviation is: 0.22561315407005234\n","List of possible accuracy for train: [0.9559742169466967, 0.9464404155817702, 0.9526895727300081, 0.9526093954879279, 0.9573663543182604, 0.9564629432044546, 0.9527662567189336, 0.9507836737745936, 0.9789027069758409, 0.9582170985512686]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 97.89027069758409 %\n","\n","Minimum Training Training Accuracy: 94.64404155817702 %\n","\n","Overall Training Accuracy: 95.62212634289754 %\n","\n","Standard Deviation is: 0.008708551224520161\n","---------------------------------------------------------------------\n","\n","green_data\n","\n","\n","List of possible accuracy for test: [0.8528376062120279, 0.8071986413116153, 0.8897097909266476, 0.9444737803122167, 0.7181716845853164, 0.9035425407336284, 0.933599720422019, 0.7837619060876599, 0.900501414041609, 0.7831103561674602]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 94.44737803122167 %\n","\n","Minimum Testing Accuracy: 71.81716845853164 %\n","\n","Overall Testing Accuracy: 85.169074408002 %\n","\n","Standard Deviation is: 0.07524781934557266\n","List of possible accuracy for train: [0.9799425186717163, 0.9799492925676094, 0.9798877347272368, 0.9766802820355494, 0.981791441741312, 0.9790685197497834, 0.9803830923660471, 0.9800926697844381, 0.9784745264332183, 0.9794929083879328]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 98.17914417413121 %\n","\n","Minimum Training Training Accuracy: 97.66802820355494 %\n","\n","Overall Training Accuracy: 97.95762986464844 %\n","\n","Standard Deviation is: 0.0013358416305404555\n","---------------------------------------------------------------------\n","\n","padparadscha_data\n","\n","\n","List of possible accuracy for test: [0.9892457416489656, 0.9296073027864551, 0.9691572068912359, 0.9504100833171736, 0.9598194254332209, 0.9189667258563464, 0.9633357318220529, 0.80536956300138, 0.7695281955210025, 0.7812842908763804]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 98.92457416489655 %\n","\n","Minimum Testing Accuracy: 76.95281955210025 %\n","\n","Overall Testing Accuracy: 90.36724267154213 %\n","\n","Standard Deviation is: 0.08436110847197817\n","List of possible accuracy for train: [0.9807625174834841, 0.9851533535675604, 0.9792285186025186, 0.979386331450243, 0.9823009041874882, 0.979231823204043, 0.9818863806940552, 0.9811422850358458, 0.9850082843331526, 0.9859986962443679]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 98.59986962443679 %\n","\n","Minimum Training Training Accuracy: 97.92285186025185 %\n","\n","Overall Training Accuracy: 98.2009909480276 %\n","\n","Standard Deviation is: 0.0025718058688024912\n","---------------------------------------------------------------------\n","\n","pink_data\n","\n","\n","List of possible accuracy for test: [0.7973361113898956, 0.9325532785781294, 0.8596810249994906, 0.8546917210920136, 0.4246868498804536, 0.6834508607617363, -0.776666901335699, 0.7779130894657493, 0.6566561249343397, 0.9203805881715683]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 93.25532785781294 %\n","\n","Minimum Testing Accuracy: -77.6666901335699 %\n","\n","Overall Testing Accuracy: 61.30682747937677 %\n","\n","Standard Deviation is: 0.5110495456086451\n","List of possible accuracy for train: [0.9416664802975734, 0.9474589400227266, 0.9238843224116473, 0.9411419576244783, 0.9743753696846726, 0.9523711320481754, 0.9444857449400038, 0.9327819534372215, 0.9472109693900457, 0.9309585290378993]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 97.43753696846727 %\n","\n","Minimum Training Training Accuracy: 92.38843224116474 %\n","\n","Overall Training Accuracy: 94.36335398894444 %\n","\n","Standard Deviation is: 0.013856748386311957\n","---------------------------------------------------------------------\n","\n","purple_data\n","\n","\n","List of possible accuracy for test: [0.323352687564486, 0.924537919032108, 0.9498177489905246, 0.727228153950967, 0.8761263676516599, 0.7528707749856866, 0.8814089450354318, 0.8678230966521263, 0.9497576083778517, 0.5420830589607182]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 94.98177489905247 %\n","\n","Minimum Testing Accuracy: 32.3352687564486 %\n","\n","Overall Testing Accuracy: 77.9500636120156 %\n","\n","Standard Deviation is: 0.20396989980542102\n","List of possible accuracy for train: [0.9607462216394292, 0.9634646489795714, 0.9445501292830937, 0.9648045034884695, 0.9538041412016351, 0.9486773507384662, 0.9523460733545778, 0.9404183535579521, 0.9618976917940385, 0.9804074097785286]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 98.04074097785286 %\n","\n","Minimum Training Training Accuracy: 94.04183535579521 %\n","\n","Overall Training Accuracy: 95.71116523815762 %\n","\n","Standard Deviation is: 0.011640900163818435\n","---------------------------------------------------------------------\n","\n","ruby_data\n","\n","\n","List of possible accuracy for test: [0.4526750352522093, 0.6853413547514422, 0.7118328440222683, 0.6801201450927806, 0.8435437672291721, 0.5878984383762104, 0.7493139164794621, 0.5789272120528668, 0.7540154768877394, 0.7656501326932548]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 84.35437672291721 %\n","\n","Minimum Testing Accuracy: 45.26750352522093 %\n","\n","Overall Testing Accuracy: 68.09318322837406 %\n","\n","Standard Deviation is: 0.11341832887881835\n","List of possible accuracy for train: [0.9486333396466334, 0.947035516400817, 0.9472090535864458, 0.9498832936686702, 0.9551848538344553, 0.949118598769877, 0.95204423895471, 0.9505863290460395, 0.9492812489215818, 0.9557188951796655]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 95.57188951796654 %\n","\n","Minimum Training Training Accuracy: 94.7035516400817 %\n","\n","Overall Training Accuracy: 95.04695368008895 %\n","\n","Standard Deviation is: 0.003012177414070655\n","---------------------------------------------------------------------\n","\n","white_data\n","\n","\n","List of possible accuracy for test: [0.9049148890109623, 0.9300119004387658, 0.9691963804439323, 0.92125743827507, -0.34055615172117415, 0.8302328091895868, 0.8558966069335631, 0.9542874712166568, 0.7308867252802571, 0.9450936991406023]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 96.91963804439322 %\n","\n","Minimum Testing Accuracy: -34.05561517211741 %\n","\n","Overall Testing Accuracy: 77.01221768208222 %\n","\n","Standard Deviation is: 0.3967723115898804\n","List of possible accuracy for train: [0.972205573761893, 0.9784190052009536, 0.9567684715937474, 0.9715950004467412, 0.9800421846913584, 0.9673853166336993, 0.9761525602522267, 0.9704454335110844, 0.9781318589357877, 0.9621080315228571]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 98.00421846913584 %\n","\n","Minimum Training Training Accuracy: 95.67684715937474 %\n","\n","Overall Training Accuracy: 97.13253436550349 %\n","\n","Standard Deviation is: 0.007527290820675293\n","---------------------------------------------------------------------\n","\n","yellow_data\n","\n","\n","List of possible accuracy for test: [0.819827048624267, 0.9190180513476345, 0.9263306070936715, 0.8687570787706623, 0.6252776480569736, 0.9262279928693107, 0.8757587795764742, 0.9475760344614246, 0.9594425396618352, 0.8967333259932387]\n","\n","Maximum Testing Accuracy That can be obtained from this model is: 95.94425396618352 %\n","\n","Minimum Testing Accuracy: 62.52776480569736 %\n","\n","Overall Testing Accuracy: 87.64949106455492 %\n","\n","Standard Deviation is: 0.09746190020315418\n","List of possible accuracy for train: [0.9876478043267967, 0.9842272467988465, 0.9826729990849552, 0.982723744581049, 0.9870790771596734, 0.983038537189546, 0.9839138507562313, 0.9817297681045193, 0.9841714832404279, 0.9835026517607316]\n","\n","Maximum Training Accuracy That can be obtained from this model is: 98.76478043267967 %\n","\n","Minimum Training Training Accuracy: 98.17297681045193 %\n","\n","Overall Training Accuracy: 98.40707163002776 %\n","\n","Standard Deviation is: 0.0019014945029073933\n","---------------------------------------------------------------------\n","\n"]}]}]}