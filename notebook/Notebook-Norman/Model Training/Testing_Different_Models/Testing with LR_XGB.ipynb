{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkxkHq1pnOf0",
        "outputId": "0d1550d4-82f0-417e-9f4e-156c1167842b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 23 is present in all training examples.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 81 is present in all training examples.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Hamming Loss: 0.04660440552805752\n",
            "Training Precision: 0.6769247854854925\n",
            "Training Recall: 0.26836367006917006\n",
            "Training F1 Score: 0.384352561858125\n",
            "Training Subset Accuracy: 0.003978006652873196\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.88      0.90     16068\n",
            "           1       0.85      0.05      0.10       201\n",
            "           2       1.00      0.00      0.00       193\n",
            "           3       0.00      0.00      0.00      1081\n",
            "           4       0.40      0.02      0.03       866\n",
            "           5       1.00      0.00      0.00       186\n",
            "           6       0.60      0.02      0.04      6670\n",
            "           7       0.50      0.04      0.08     12940\n",
            "           8       0.51      0.05      0.09      5940\n",
            "           9       0.08      0.00      0.00      1233\n",
            "          10       0.00      0.00      0.00      1882\n",
            "          11       0.00      0.00      0.00       367\n",
            "          12       0.15      0.00      0.01     11988\n",
            "          13       0.00      0.00      0.00      5286\n",
            "          14       0.00      0.00      0.00       464\n",
            "          15       0.00      0.00      0.00      4516\n",
            "          16       0.21      0.01      0.02     12677\n",
            "          17       0.00      0.00      0.00       887\n",
            "          18       0.49      0.09      0.15     19914\n",
            "          19       0.04      0.00      0.00      3630\n",
            "          20       0.64      0.53      0.58     41262\n",
            "          21       0.59      0.02      0.04      6601\n",
            "          22       0.76      0.04      0.08       320\n",
            "          23       1.00      1.00      1.00         0\n",
            "          24       0.43      0.03      0.06      5656\n",
            "          25       0.31      0.02      0.04       192\n",
            "          26       0.00      0.00      0.00       206\n",
            "          27       1.00      0.00      0.00        43\n",
            "          28       0.70      0.68      0.69     44284\n",
            "          29       0.00      0.00      0.00       763\n",
            "          30       0.00      0.00      0.00       338\n",
            "          31       0.00      0.00      0.00      4429\n",
            "          32       0.00      0.00      0.00      1137\n",
            "          33       0.00      0.00      0.00       849\n",
            "          34       0.00      0.00      0.00       617\n",
            "          35       0.19      0.01      0.02       304\n",
            "          36       0.15      0.00      0.01     22151\n",
            "          37       0.51      0.02      0.04      1503\n",
            "          38       0.07      0.00      0.01      6837\n",
            "          39       0.14      0.01      0.02     11656\n",
            "          40       0.06      0.00      0.01      2927\n",
            "          41       0.56      0.14      0.23       576\n",
            "          42       0.69      0.35      0.46     18279\n",
            "          43       0.67      0.37      0.48     12939\n",
            "          44       0.00      0.00      0.00       353\n",
            "          45       0.00      0.00      0.00      3401\n",
            "          46       0.15      0.01      0.01      4622\n",
            "          47       0.00      0.00      0.00      1127\n",
            "          48       0.00      0.00      0.00       112\n",
            "          49       0.10      0.00      0.01      3773\n",
            "          50       0.26      0.01      0.02      2115\n",
            "          51       0.71      0.53      0.61     15459\n",
            "          52       1.00      0.00      0.00       389\n",
            "          53       0.24      0.02      0.03      4639\n",
            "          54       0.22      0.01      0.01      1738\n",
            "          55       0.94      0.62      0.74        26\n",
            "          56       0.00      0.00      0.00       994\n",
            "          57       0.10      0.01      0.02       516\n",
            "          58       0.31      0.03      0.06      8808\n",
            "          59       0.00      0.00      0.00      1870\n",
            "          60       1.00      0.00      0.00        91\n",
            "          61       1.00      0.00      0.00        38\n",
            "          62       0.00      0.00      0.00      2292\n",
            "          63       1.00      0.00      0.00        58\n",
            "          64       0.12      0.00      0.01      4194\n",
            "          65       1.00      0.00      0.00        75\n",
            "          66       0.94      0.62      0.74        26\n",
            "          67       0.19      0.01      0.02      4310\n",
            "          68       0.52      0.20      0.29     12854\n",
            "          69       0.00      0.00      0.00       288\n",
            "          70       0.00      0.00      0.00       416\n",
            "          71       0.94      0.62      0.74        26\n",
            "          72       0.03      0.00      0.00      1351\n",
            "          73       0.73      0.53      0.62     18590\n",
            "          74       1.00      0.00      0.00       141\n",
            "          75       1.00      0.00      0.00        91\n",
            "          76       1.00      0.00      0.00        38\n",
            "          77       0.18      0.01      0.01       460\n",
            "          78       1.00      0.00      0.00        58\n",
            "          79       0.00      0.00      0.00       116\n",
            "          80       0.31      0.02      0.04      7701\n",
            "          81       1.00      1.00      1.00         0\n",
            "          82       0.70      0.42      0.53     13642\n",
            "          83       0.15      0.00      0.01      2099\n",
            "          84       0.08      0.00      0.01      2394\n",
            "          85       0.69      0.55      0.62     13509\n",
            "          86       0.00      0.00      0.00       835\n",
            "          87       0.20      0.01      0.02      4370\n",
            "          88       0.94      0.08      0.14       606\n",
            "          89       0.67      0.23      0.34        44\n",
            "          90       1.00      0.00      0.00        32\n",
            "\n",
            "   micro avg       0.68      0.27      0.38    431545\n",
            "   macro avg       0.40      0.11      0.13    431545\n",
            "weighted avg       0.46      0.27      0.31    431545\n",
            " samples avg       0.77      0.28      0.34    431545\n",
            "\n",
            "================================================================================\n",
            "Validation Hamming Loss: 0.04667387116147419\n",
            "Validation Precision: 0.6750575155613169\n",
            "Validation Recall: 0.26791110493140524\n",
            "Validation F1 Score: 0.38358732402310636\n",
            "Validation Subset Accuracy: 0.0036350750448668856\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.88      0.90     15732\n",
            "           1       0.73      0.04      0.07       209\n",
            "           2       1.00      0.00      0.00       162\n",
            "           3       1.00      0.00      0.00      1073\n",
            "           4       0.30      0.01      0.03       868\n",
            "           5       1.00      0.00      0.00       171\n",
            "           6       0.53      0.02      0.04      6859\n",
            "           7       0.51      0.04      0.08     12841\n",
            "           8       0.48      0.05      0.09      5710\n",
            "           9       0.00      0.00      0.00      1290\n",
            "          10       0.00      0.00      0.00      1880\n",
            "          11       0.00      0.00      0.00       349\n",
            "          12       0.12      0.00      0.01     12096\n",
            "          13       0.00      0.00      0.00      5294\n",
            "          14       0.00      0.00      0.00       439\n",
            "          15       0.00      0.00      0.00      4496\n",
            "          16       0.27      0.01      0.02     12415\n",
            "          17       0.00      0.00      0.00       892\n",
            "          18       0.49      0.09      0.16     19442\n",
            "          19       0.01      0.00      0.00      3644\n",
            "          20       0.65      0.53      0.58     41424\n",
            "          21       0.52      0.02      0.04      6783\n",
            "          22       0.53      0.03      0.05       315\n",
            "          23       1.00      1.00      1.00         0\n",
            "          24       0.43      0.03      0.05      5428\n",
            "          25       0.14      0.01      0.01       179\n",
            "          26       0.00      0.00      0.00       216\n",
            "          27       1.00      0.00      0.00        39\n",
            "          28       0.70      0.68      0.69     44088\n",
            "          29       0.00      0.00      0.00       699\n",
            "          30       0.00      0.00      0.00       335\n",
            "          31       0.00      0.00      0.00      4415\n",
            "          32       0.00      0.00      0.00      1186\n",
            "          33       0.00      0.00      0.00       851\n",
            "          34       0.00      0.00      0.00       574\n",
            "          35       0.13      0.01      0.01       281\n",
            "          36       0.13      0.00      0.01     22448\n",
            "          37       0.38      0.02      0.03      1523\n",
            "          38       0.09      0.01      0.01      6871\n",
            "          39       0.13      0.01      0.02     11257\n",
            "          40       0.09      0.00      0.01      2982\n",
            "          41       0.52      0.13      0.21       610\n",
            "          42       0.69      0.35      0.46     18015\n",
            "          43       0.68      0.37      0.48     13062\n",
            "          44       0.00      0.00      0.00       298\n",
            "          45       0.00      0.00      0.00      3379\n",
            "          46       0.11      0.01      0.01      4612\n",
            "          47       0.00      0.00      0.00      1218\n",
            "          48       0.00      0.00      0.00        98\n",
            "          49       0.07      0.00      0.00      3893\n",
            "          50       0.32      0.01      0.02      2146\n",
            "          51       0.70      0.53      0.60     15606\n",
            "          52       1.00      0.00      0.00       409\n",
            "          53       0.25      0.02      0.04      4727\n",
            "          54       0.25      0.01      0.01      1803\n",
            "          55       0.96      0.71      0.81        34\n",
            "          56       0.05      0.00      0.00      1058\n",
            "          57       0.13      0.02      0.04       532\n",
            "          58       0.28      0.03      0.06      8868\n",
            "          59       0.00      0.00      0.00      1847\n",
            "          60       1.00      0.00      0.00        83\n",
            "          61       0.00      0.00      0.00        40\n",
            "          62       0.00      0.00      0.00      2275\n",
            "          63       1.00      0.00      0.00        67\n",
            "          64       0.11      0.00      0.01      4200\n",
            "          65       1.00      0.00      0.00        76\n",
            "          66       0.96      0.71      0.81        34\n",
            "          67       0.11      0.01      0.01      4341\n",
            "          68       0.53      0.20      0.29     13092\n",
            "          69       0.00      0.00      0.00       317\n",
            "          70       0.00      0.00      0.00       438\n",
            "          71       0.96      0.71      0.81        34\n",
            "          72       0.08      0.00      0.00      1347\n",
            "          73       0.72      0.53      0.61     18778\n",
            "          74       1.00      0.00      0.00       130\n",
            "          75       1.00      0.00      0.00        83\n",
            "          76       0.00      0.00      0.00        40\n",
            "          77       0.19      0.01      0.01       509\n",
            "          78       1.00      0.00      0.00        67\n",
            "          79       0.00      0.00      0.00       109\n",
            "          80       0.28      0.02      0.04      8047\n",
            "          81       1.00      0.00      0.00         1\n",
            "          82       0.70      0.42      0.53     13586\n",
            "          83       0.10      0.00      0.00      2072\n",
            "          84       0.06      0.00      0.01      2291\n",
            "          85       0.70      0.57      0.62     13330\n",
            "          86       0.00      0.00      0.00       869\n",
            "          87       0.22      0.01      0.02      4547\n",
            "          88       0.90      0.08      0.15       668\n",
            "          89       0.83      0.37      0.51        51\n",
            "          90       1.00      0.00      0.00        27\n",
            "\n",
            "   micro avg       0.68      0.27      0.38    431520\n",
            "   macro avg       0.38      0.10      0.12    431520\n",
            "weighted avg       0.46      0.27      0.30    431520\n",
            " samples avg       0.77      0.28      0.34    431520\n",
            "\n",
            "================================================================================\n",
            "Testing Hamming Loss: 0.04670285704906424\n",
            "Testing Precision: 0.6735411072970126\n",
            "Testing Recall: 0.2679937253204955\n",
            "Testing F1 Score: 0.38342668640140853\n",
            "Testing Subset Accuracy: 0.003614104342259682\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.88      0.90     13517\n",
            "           1       0.57      0.02      0.04       189\n",
            "           2       1.00      0.00      0.00       156\n",
            "           3       1.00      0.00      0.00       982\n",
            "           4       0.34      0.01      0.03       712\n",
            "           5       1.00      0.00      0.00       175\n",
            "           6       0.49      0.02      0.03      5710\n",
            "           7       0.52      0.04      0.08     10951\n",
            "           8       0.50      0.05      0.09      5109\n",
            "           9       0.00      0.00      0.00      1013\n",
            "          10       0.00      0.00      0.00      1609\n",
            "          11       0.00      0.00      0.00       290\n",
            "          12       0.18      0.01      0.01     10479\n",
            "          13       0.00      0.00      0.00      4572\n",
            "          14       1.00      0.00      0.00       385\n",
            "          15       0.00      0.00      0.00      3910\n",
            "          16       0.25      0.01      0.02     10742\n",
            "          17       0.00      0.00      0.00       727\n",
            "          18       0.48      0.09      0.15     16873\n",
            "          19       0.00      0.00      0.00      3181\n",
            "          20       0.64      0.53      0.58     35391\n",
            "          21       0.47      0.02      0.03      5658\n",
            "          22       0.64      0.02      0.05       287\n",
            "          23       1.00      0.00      0.00         1\n",
            "          24       0.46      0.03      0.06      4870\n",
            "          25       0.33      0.01      0.03       149\n",
            "          26       0.00      0.00      0.00       176\n",
            "          27       0.00      0.00      0.00        27\n",
            "          28       0.70      0.68      0.69     37933\n",
            "          29       1.00      0.00      0.00       604\n",
            "          30       0.00      0.00      0.00       283\n",
            "          31       0.00      0.00      0.00      3837\n",
            "          32       0.00      0.00      0.00       941\n",
            "          33       0.00      0.00      0.00       694\n",
            "          34       0.00      0.00      0.00       503\n",
            "          35       0.38      0.02      0.04       250\n",
            "          36       0.15      0.00      0.01     19005\n",
            "          37       0.59      0.02      0.04      1314\n",
            "          38       0.08      0.00      0.01      5962\n",
            "          39       0.12      0.01      0.02      9795\n",
            "          40       0.08      0.00      0.01      2516\n",
            "          41       0.37      0.09      0.15       467\n",
            "          42       0.69      0.34      0.46     15685\n",
            "          43       0.67      0.37      0.47     11009\n",
            "          44       0.00      0.00      0.00       277\n",
            "          45       0.00      0.00      0.00      2837\n",
            "          46       0.13      0.01      0.01      4041\n",
            "          47       0.00      0.00      0.00      1009\n",
            "          48       0.00      0.00      0.00        92\n",
            "          49       0.08      0.00      0.01      3329\n",
            "          50       0.28      0.01      0.02      1800\n",
            "          51       0.69      0.53      0.60     13230\n",
            "          52       1.00      0.00      0.00       298\n",
            "          53       0.25      0.02      0.04      3956\n",
            "          54       0.27      0.01      0.01      1494\n",
            "          55       1.00      0.50      0.67        34\n",
            "          56       0.07      0.00      0.00       899\n",
            "          57       0.09      0.01      0.02       453\n",
            "          58       0.29      0.03      0.06      7569\n",
            "          59       0.00      0.00      0.00      1634\n",
            "          60       1.00      0.00      0.00        71\n",
            "          61       1.00      0.00      0.00        31\n",
            "          62       0.00      0.00      0.00      1956\n",
            "          63       1.00      0.00      0.00        56\n",
            "          64       0.12      0.00      0.01      3561\n",
            "          65       1.00      0.00      0.00        63\n",
            "          66       1.00      0.50      0.67        34\n",
            "          67       0.14      0.01      0.01      3755\n",
            "          68       0.53      0.21      0.30     11038\n",
            "          69       0.00      0.00      0.00       224\n",
            "          70       0.00      0.00      0.00       362\n",
            "          71       1.00      0.50      0.67        34\n",
            "          72       0.09      0.00      0.00      1119\n",
            "          73       0.72      0.54      0.62     16010\n",
            "          74       1.00      0.00      0.00       127\n",
            "          75       1.00      0.00      0.00        71\n",
            "          76       1.00      0.00      0.00        30\n",
            "          77       0.00      0.00      0.00       418\n",
            "          78       1.00      0.00      0.00        56\n",
            "          79       0.00      0.00      0.00        93\n",
            "          80       0.29      0.02      0.04      6722\n",
            "          81       1.00      1.00      1.00         0\n",
            "          82       0.70      0.43      0.54     11636\n",
            "          83       0.26      0.01      0.01      1809\n",
            "          84       0.06      0.00      0.00      2092\n",
            "          85       0.69      0.56      0.61     11517\n",
            "          86       0.00      0.00      0.00       698\n",
            "          87       0.22      0.01      0.02      3856\n",
            "          88       0.98      0.09      0.16       621\n",
            "          89       0.86      0.37      0.51        52\n",
            "          90       1.00      0.00      0.00        37\n",
            "\n",
            "   micro avg       0.67      0.27      0.38    369740\n",
            "   macro avg       0.42      0.10      0.12    369740\n",
            "weighted avg       0.46      0.27      0.30    369740\n",
            " samples avg       0.77      0.28      0.34    369740\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, hamming_loss, precision_score, recall_score,\n",
        "    f1_score, classification_report\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "file_path = r\"final_merged.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "X = df.iloc[:, 92:110].values\n",
        "y = df.iloc[:, 1:92].values\n",
        "\n",
        "\n",
        "y = np.nan_to_num(y, nan=0)\n",
        "\n",
        "X = np.nan_to_num(X, nan=0)\n",
        "\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize OneVsRestClassifier with Logistic Regression\n",
        "logistic_model = OneVsRestClassifier(\n",
        "    LogisticRegression(max_iter=500, solver='lbfgs', n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(X, y, dataset_name):\n",
        "    y_pred = logistic_model.predict(X)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    hamming = hamming_loss(y, y_pred)\n",
        "    precision = precision_score(y, y_pred, average='micro', zero_division=1)\n",
        "    recall = recall_score(y, y_pred, average='micro', zero_division=1)\n",
        "    f1 = f1_score(y, y_pred, average='micro', zero_division=1)\n",
        "    subset_accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y, y_pred, zero_division=1)\n",
        "\n",
        "    print(f\"{dataset_name} Hamming Loss: {hamming}\")\n",
        "    print(f\"{dataset_name} Precision: {precision}\")\n",
        "    print(f\"{dataset_name} Recall: {recall}\")\n",
        "    print(f\"{dataset_name} F1 Score: {f1}\")\n",
        "    print(f\"{dataset_name} Subset Accuracy: {subset_accuracy}\")\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(report)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "evaluate_model(X_train, y_train, \"Training\")\n",
        "evaluate_model(X_val, y_val, \"Validation\")\n",
        "evaluate_model(X_test, y_test, \"Testing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, hamming_loss, precision_score, recall_score,\n",
        "    f1_score, classification_report\n",
        ")\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "file_path = r\"final_merged.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = df.iloc[:, 92:110].values  # Features\n",
        "y = df.iloc[:, 1:92].values  # Targets (binary multi-label data)\n",
        "\n",
        "# Fill NaN values in target columns with 0\n",
        "y = np.nan_to_num(y, nan=0)\n",
        "\n",
        "\n",
        "# First split: Separate test set (30%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Second split: Separate train and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize MultiOutputClassifier with XGBoost\n",
        "xgb_model = MultiOutputClassifier(\n",
        "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(X, y, dataset_name):\n",
        "    y_pred = xgb_model.predict(X)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    hamming = hamming_loss(y, y_pred)\n",
        "    precision = precision_score(y, y_pred, average='micro', zero_division=1)\n",
        "    recall = recall_score(y, y_pred, average='micro', zero_division=1)\n",
        "    f1 = f1_score(y, y_pred, average='micro', zero_division=1)\n",
        "    subset_accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y, y_pred, zero_division=1)\n",
        "\n",
        "    print(f\"{dataset_name} Hamming Loss: {hamming}\")\n",
        "    print(f\"{dataset_name} Precision: {precision}\")\n",
        "    print(f\"{dataset_name} Recall: {recall}\")\n",
        "    print(f\"{dataset_name} F1 Score: {f1}\")\n",
        "    print(f\"{dataset_name} Subset Accuracy: {subset_accuracy}\")\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(report)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "evaluate_model(X_train, y_train, \"Training\")\n",
        "evaluate_model(X_val, y_val, \"Validation\")\n",
        "evaluate_model(X_test, y_test, \"Testing\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83qEeEMQyOcS",
        "outputId": "7743bff1-a7bb-4395-acf1-76c9bf728cc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:36:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:37:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:38:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Hamming Loss: 0.007577281145255905\n",
            "Training Precision: 0.9502085960997381\n",
            "Training Recall: 0.9077894541704805\n",
            "Training F1 Score: 0.9285147986845613\n",
            "Training Subset Accuracy: 0.6164995827665436\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     16068\n",
            "           1       1.00      1.00      1.00       201\n",
            "           2       1.00      0.99      1.00       193\n",
            "           3       1.00      1.00      1.00      1081\n",
            "           4       1.00      1.00      1.00       866\n",
            "           5       1.00      0.99      1.00       186\n",
            "           6       0.98      0.85      0.91      6670\n",
            "           7       0.95      0.69      0.80     12940\n",
            "           8       0.99      0.87      0.93      5940\n",
            "           9       1.00      1.00      1.00      1233\n",
            "          10       1.00      0.94      0.97      1882\n",
            "          11       1.00      1.00      1.00       367\n",
            "          12       0.96      0.74      0.84     11988\n",
            "          13       0.99      0.87      0.93      5286\n",
            "          14       1.00      1.00      1.00       464\n",
            "          15       0.99      0.80      0.89      4516\n",
            "          16       0.96      0.85      0.90     12677\n",
            "          17       1.00      1.00      1.00       887\n",
            "          18       0.91      0.78      0.84     19914\n",
            "          19       1.00      0.90      0.94      3630\n",
            "          20       0.87      0.94      0.91     41262\n",
            "          21       0.98      0.85      0.91      6601\n",
            "          22       1.00      1.00      1.00       320\n",
            "          23       1.00      1.00      1.00         0\n",
            "          24       0.98      0.87      0.92      5656\n",
            "          25       1.00      1.00      1.00       192\n",
            "          26       1.00      1.00      1.00       206\n",
            "          27       1.00      1.00      1.00        43\n",
            "          28       0.91      0.96      0.94     44284\n",
            "          29       1.00      1.00      1.00       763\n",
            "          30       1.00      1.00      1.00       338\n",
            "          31       0.99      0.84      0.91      4429\n",
            "          32       1.00      1.00      1.00      1137\n",
            "          33       1.00      1.00      1.00       849\n",
            "          34       1.00      1.00      1.00       617\n",
            "          35       1.00      1.00      1.00       304\n",
            "          36       0.89      0.83      0.86     22151\n",
            "          37       1.00      1.00      1.00      1503\n",
            "          38       0.98      0.84      0.91      6837\n",
            "          39       0.95      0.80      0.87     11656\n",
            "          40       1.00      0.95      0.97      2927\n",
            "          41       1.00      1.00      1.00       576\n",
            "          42       0.96      0.89      0.92     18279\n",
            "          43       0.95      0.96      0.95     12939\n",
            "          44       1.00      1.00      1.00       353\n",
            "          45       0.99      0.92      0.96      3401\n",
            "          46       0.99      0.87      0.93      4622\n",
            "          47       1.00      1.00      1.00      1127\n",
            "          48       1.00      1.00      1.00       112\n",
            "          49       0.99      0.89      0.94      3773\n",
            "          50       1.00      0.95      0.97      2115\n",
            "          51       0.96      0.99      0.97     15459\n",
            "          52       1.00      0.99      1.00       389\n",
            "          53       0.99      0.89      0.94      4639\n",
            "          54       1.00      0.98      0.99      1738\n",
            "          55       1.00      1.00      1.00        26\n",
            "          56       1.00      0.99      1.00       994\n",
            "          57       1.00      1.00      1.00       516\n",
            "          58       0.96      0.93      0.94      8808\n",
            "          59       1.00      0.97      0.98      1870\n",
            "          60       1.00      0.91      0.95        91\n",
            "          61       1.00      0.92      0.96        38\n",
            "          62       1.00      0.94      0.97      2292\n",
            "          63       1.00      0.74      0.85        58\n",
            "          64       0.98      0.89      0.93      4194\n",
            "          65       1.00      0.97      0.99        75\n",
            "          66       1.00      1.00      1.00        26\n",
            "          67       0.99      0.86      0.92      4310\n",
            "          68       0.94      0.95      0.94     12854\n",
            "          69       1.00      0.99      1.00       288\n",
            "          70       1.00      1.00      1.00       416\n",
            "          71       1.00      1.00      1.00        26\n",
            "          72       1.00      0.99      0.99      1351\n",
            "          73       0.97      0.99      0.98     18590\n",
            "          74       1.00      1.00      1.00       141\n",
            "          75       1.00      0.91      0.95        91\n",
            "          76       1.00      0.92      0.96        38\n",
            "          77       1.00      1.00      1.00       460\n",
            "          78       1.00      0.74      0.85        58\n",
            "          79       1.00      1.00      1.00       116\n",
            "          80       0.97      0.95      0.96      7701\n",
            "          81       1.00      1.00      1.00         0\n",
            "          82       0.97      0.98      0.97     13642\n",
            "          83       1.00      0.99      0.99      2099\n",
            "          84       1.00      0.99      0.99      2394\n",
            "          85       0.98      0.98      0.98     13509\n",
            "          86       1.00      1.00      1.00       835\n",
            "          87       1.00      0.98      0.99      4370\n",
            "          88       1.00      1.00      1.00       606\n",
            "          89       1.00      1.00      1.00        44\n",
            "          90       1.00      1.00      1.00        32\n",
            "\n",
            "   micro avg       0.95      0.91      0.93    431545\n",
            "   macro avg       0.99      0.94      0.96    431545\n",
            "weighted avg       0.95      0.91      0.93    431545\n",
            " samples avg       0.95      0.91      0.92    431545\n",
            "\n",
            "================================================================================\n",
            "Validation Hamming Loss: 0.01413543989646229\n",
            "Validation Precision: 0.9088611294114028\n",
            "Validation Recall: 0.821616611049314\n",
            "Validation F1 Score: 0.8630395914398071\n",
            "Validation Subset Accuracy: 0.4286187857934866\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99     15732\n",
            "           1       0.97      0.72      0.83       209\n",
            "           2       0.98      0.73      0.83       162\n",
            "           3       0.99      0.87      0.93      1073\n",
            "           4       0.99      0.94      0.96       868\n",
            "           5       0.99      0.96      0.98       171\n",
            "           6       0.94      0.70      0.80      6859\n",
            "           7       0.91      0.59      0.72     12841\n",
            "           8       0.94      0.71      0.81      5710\n",
            "           9       0.98      0.85      0.91      1290\n",
            "          10       0.97      0.71      0.82      1880\n",
            "          11       0.99      0.87      0.92       349\n",
            "          12       0.91      0.63      0.75     12096\n",
            "          13       0.95      0.73      0.83      5294\n",
            "          14       1.00      0.86      0.93       439\n",
            "          15       0.96      0.63      0.76      4496\n",
            "          16       0.91      0.75      0.82     12415\n",
            "          17       0.95      0.79      0.87       892\n",
            "          18       0.86      0.69      0.76     19442\n",
            "          19       0.98      0.72      0.83      3644\n",
            "          20       0.84      0.91      0.87     41424\n",
            "          21       0.93      0.70      0.80      6783\n",
            "          22       1.00      0.73      0.84       315\n",
            "          23       1.00      1.00      1.00         0\n",
            "          24       0.94      0.69      0.80      5428\n",
            "          25       0.99      0.74      0.85       179\n",
            "          26       1.00      0.80      0.89       216\n",
            "          27       1.00      0.79      0.89        39\n",
            "          28       0.88      0.94      0.91     44088\n",
            "          29       0.97      0.76      0.85       699\n",
            "          30       0.99      0.82      0.89       335\n",
            "          31       0.96      0.65      0.78      4415\n",
            "          32       0.99      0.76      0.86      1186\n",
            "          33       0.95      0.77      0.85       851\n",
            "          34       0.99      0.89      0.94       574\n",
            "          35       1.00      0.86      0.92       281\n",
            "          36       0.84      0.77      0.81     22448\n",
            "          37       0.98      0.82      0.89      1523\n",
            "          38       0.94      0.70      0.81      6871\n",
            "          39       0.90      0.68      0.78     11257\n",
            "          40       0.97      0.76      0.85      2982\n",
            "          41       0.99      0.93      0.96       610\n",
            "          42       0.92      0.83      0.87     18015\n",
            "          43       0.90      0.89      0.90     13062\n",
            "          44       0.97      0.85      0.91       298\n",
            "          45       0.96      0.76      0.85      3379\n",
            "          46       0.94      0.68      0.79      4612\n",
            "          47       0.98      0.80      0.88      1218\n",
            "          48       1.00      0.87      0.93        98\n",
            "          49       0.95      0.67      0.79      3893\n",
            "          50       0.96      0.68      0.80      2146\n",
            "          51       0.91      0.95      0.93     15606\n",
            "          52       0.96      0.64      0.77       409\n",
            "          53       0.94      0.71      0.81      4727\n",
            "          54       0.94      0.75      0.83      1803\n",
            "          55       1.00      0.82      0.90        34\n",
            "          56       0.98      0.75      0.85      1058\n",
            "          57       0.98      0.88      0.93       532\n",
            "          58       0.90      0.82      0.86      8868\n",
            "          59       0.97      0.73      0.83      1847\n",
            "          60       1.00      0.71      0.83        83\n",
            "          61       0.96      0.60      0.74        40\n",
            "          62       0.96      0.71      0.82      2275\n",
            "          63       1.00      0.54      0.70        67\n",
            "          64       0.92      0.71      0.80      4200\n",
            "          65       1.00      0.68      0.81        76\n",
            "          66       1.00      0.82      0.90        34\n",
            "          67       0.93      0.67      0.78      4341\n",
            "          68       0.87      0.88      0.88     13092\n",
            "          69       0.97      0.61      0.75       317\n",
            "          70       1.00      0.80      0.89       438\n",
            "          71       1.00      0.82      0.90        34\n",
            "          72       0.96      0.73      0.83      1347\n",
            "          73       0.93      0.97      0.95     18778\n",
            "          74       0.99      0.71      0.83       130\n",
            "          75       1.00      0.71      0.83        83\n",
            "          76       0.96      0.60      0.74        40\n",
            "          77       0.98      0.69      0.81       509\n",
            "          78       1.00      0.54      0.70        67\n",
            "          79       1.00      0.77      0.87       109\n",
            "          80       0.93      0.85      0.89      8047\n",
            "          81       1.00      0.00      0.00         1\n",
            "          82       0.93      0.95      0.94     13586\n",
            "          83       0.97      0.82      0.89      2072\n",
            "          84       0.96      0.84      0.90      2291\n",
            "          85       0.95      0.95      0.95     13330\n",
            "          86       0.99      0.89      0.94       869\n",
            "          87       0.96      0.91      0.93      4547\n",
            "          88       0.99      0.84      0.91       668\n",
            "          89       1.00      0.80      0.89        51\n",
            "          90       0.95      0.74      0.83        27\n",
            "\n",
            "   micro avg       0.91      0.82      0.86    431520\n",
            "   macro avg       0.96      0.77      0.85    431520\n",
            "weighted avg       0.91      0.82      0.86    431520\n",
            " samples avg       0.90      0.82      0.84    431520\n",
            "\n",
            "================================================================================\n",
            "Testing Hamming Loss: 0.014060435457000057\n",
            "Testing Precision: 0.909955799913756\n",
            "Testing Recall: 0.8218396711202467\n",
            "Testing F1 Score: 0.863656000773083\n",
            "Testing Subset Accuracy: 0.43050517443721326\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99     13517\n",
            "           1       0.97      0.78      0.86       189\n",
            "           2       1.00      0.74      0.85       156\n",
            "           3       1.00      0.87      0.93       982\n",
            "           4       0.99      0.93      0.96       712\n",
            "           5       1.00      0.94      0.97       175\n",
            "           6       0.94      0.71      0.81      5710\n",
            "           7       0.90      0.59      0.71     10951\n",
            "           8       0.94      0.70      0.81      5109\n",
            "           9       0.98      0.87      0.92      1013\n",
            "          10       0.98      0.72      0.83      1609\n",
            "          11       1.00      0.87      0.93       290\n",
            "          12       0.92      0.64      0.75     10479\n",
            "          13       0.96      0.71      0.82      4572\n",
            "          14       1.00      0.85      0.92       385\n",
            "          15       0.96      0.63      0.76      3910\n",
            "          16       0.92      0.75      0.83     10742\n",
            "          17       0.94      0.78      0.86       727\n",
            "          18       0.86      0.70      0.77     16873\n",
            "          19       0.98      0.71      0.83      3181\n",
            "          20       0.84      0.91      0.88     35391\n",
            "          21       0.94      0.71      0.81      5658\n",
            "          22       1.00      0.75      0.86       287\n",
            "          23       1.00      0.00      0.00         1\n",
            "          24       0.94      0.69      0.79      4870\n",
            "          25       1.00      0.74      0.85       149\n",
            "          26       0.98      0.80      0.88       176\n",
            "          27       1.00      0.78      0.88        27\n",
            "          28       0.89      0.94      0.91     37933\n",
            "          29       0.98      0.78      0.87       604\n",
            "          30       0.99      0.80      0.89       283\n",
            "          31       0.96      0.66      0.78      3837\n",
            "          32       0.98      0.77      0.86       941\n",
            "          33       0.94      0.76      0.84       694\n",
            "          34       0.98      0.85      0.91       503\n",
            "          35       1.00      0.86      0.92       250\n",
            "          36       0.84      0.77      0.80     19005\n",
            "          37       0.98      0.83      0.90      1314\n",
            "          38       0.94      0.71      0.81      5962\n",
            "          39       0.90      0.68      0.78      9795\n",
            "          40       0.97      0.76      0.85      2516\n",
            "          41       1.00      0.95      0.97       467\n",
            "          42       0.93      0.83      0.88     15685\n",
            "          43       0.91      0.90      0.90     11009\n",
            "          44       0.97      0.84      0.91       277\n",
            "          45       0.95      0.76      0.84      2837\n",
            "          46       0.95      0.67      0.79      4041\n",
            "          47       0.98      0.80      0.88      1009\n",
            "          48       0.99      0.86      0.92        92\n",
            "          49       0.95      0.69      0.80      3329\n",
            "          50       0.95      0.69      0.80      1800\n",
            "          51       0.92      0.95      0.93     13230\n",
            "          52       0.98      0.58      0.73       298\n",
            "          53       0.93      0.71      0.80      3956\n",
            "          54       0.94      0.72      0.81      1494\n",
            "          55       1.00      0.74      0.85        34\n",
            "          56       0.98      0.73      0.83       899\n",
            "          57       0.97      0.85      0.91       453\n",
            "          58       0.91      0.83      0.87      7569\n",
            "          59       0.97      0.72      0.83      1634\n",
            "          60       1.00      0.68      0.81        71\n",
            "          61       1.00      0.48      0.65        31\n",
            "          62       0.95      0.71      0.81      1956\n",
            "          63       1.00      0.71      0.83        56\n",
            "          64       0.91      0.69      0.79      3561\n",
            "          65       0.98      0.75      0.85        63\n",
            "          66       1.00      0.74      0.85        34\n",
            "          67       0.93      0.68      0.78      3755\n",
            "          68       0.87      0.88      0.88     11038\n",
            "          69       0.98      0.61      0.75       224\n",
            "          70       1.00      0.80      0.89       362\n",
            "          71       1.00      0.74      0.85        34\n",
            "          72       0.96      0.70      0.81      1119\n",
            "          73       0.93      0.97      0.95     16010\n",
            "          74       0.99      0.74      0.85       127\n",
            "          75       1.00      0.68      0.81        71\n",
            "          76       1.00      0.50      0.67        30\n",
            "          77       0.98      0.67      0.79       418\n",
            "          78       1.00      0.71      0.83        56\n",
            "          79       1.00      0.81      0.89        93\n",
            "          80       0.92      0.85      0.88      6722\n",
            "          81       1.00      1.00      1.00         0\n",
            "          82       0.93      0.95      0.94     11636\n",
            "          83       0.97      0.81      0.88      1809\n",
            "          84       0.95      0.84      0.89      2092\n",
            "          85       0.95      0.95      0.95     11517\n",
            "          86       0.99      0.89      0.93       698\n",
            "          87       0.96      0.91      0.93      3856\n",
            "          88       0.99      0.85      0.92       621\n",
            "          89       1.00      0.90      0.95        52\n",
            "          90       1.00      0.65      0.79        37\n",
            "\n",
            "   micro avg       0.91      0.82      0.86    369740\n",
            "   macro avg       0.96      0.77      0.84    369740\n",
            "weighted avg       0.91      0.82      0.86    369740\n",
            " samples avg       0.90      0.82      0.85    369740\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}