{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2440f365-89cc-4124-a92a-e44d71695f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hamming Loss: 0.0001336553959409208\n",
      "Training Precision: 0.9993921184209\n",
      "Training Recall: 0.9981415611349918\n",
      "Training F1 Score: 0.9987664483218364\n",
      "Training Subset Accuracy: 0.9968107360455413\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     16068\n",
      "           1       1.00      1.00      1.00       201\n",
      "           2       1.00      0.99      1.00       193\n",
      "           3       1.00      1.00      1.00      1081\n",
      "           4       1.00      1.00      1.00       866\n",
      "           5       1.00      0.99      1.00       186\n",
      "           6       1.00      1.00      1.00      6670\n",
      "           7       1.00      1.00      1.00     12940\n",
      "           8       1.00      1.00      1.00      5940\n",
      "           9       1.00      1.00      1.00      1233\n",
      "          10       1.00      1.00      1.00      1882\n",
      "          11       1.00      1.00      1.00       367\n",
      "          12       1.00      1.00      1.00     11988\n",
      "          13       1.00      1.00      1.00      5286\n",
      "          14       1.00      1.00      1.00       464\n",
      "          15       1.00      1.00      1.00      4516\n",
      "          16       1.00      1.00      1.00     12677\n",
      "          17       1.00      1.00      1.00       887\n",
      "          18       1.00      1.00      1.00     19914\n",
      "          19       1.00      1.00      1.00      3630\n",
      "          20       1.00      1.00      1.00     41262\n",
      "          21       1.00      1.00      1.00      6601\n",
      "          22       1.00      1.00      1.00       320\n",
      "          23       1.00      1.00      1.00         0\n",
      "          24       1.00      1.00      1.00      5656\n",
      "          25       1.00      1.00      1.00       192\n",
      "          26       1.00      1.00      1.00       206\n",
      "          27       1.00      1.00      1.00        43\n",
      "          28       1.00      1.00      1.00     44284\n",
      "          29       1.00      1.00      1.00       763\n",
      "          30       1.00      1.00      1.00       338\n",
      "          31       1.00      1.00      1.00      4429\n",
      "          32       1.00      1.00      1.00      1137\n",
      "          33       1.00      1.00      1.00       849\n",
      "          34       1.00      1.00      1.00       617\n",
      "          35       1.00      1.00      1.00       304\n",
      "          36       1.00      1.00      1.00     22151\n",
      "          37       1.00      1.00      1.00      1503\n",
      "          38       1.00      1.00      1.00      6837\n",
      "          39       1.00      1.00      1.00     11656\n",
      "          40       1.00      1.00      1.00      2927\n",
      "          41       1.00      1.00      1.00       576\n",
      "          42       1.00      1.00      1.00     18279\n",
      "          43       1.00      1.00      1.00     12939\n",
      "          44       1.00      1.00      1.00       353\n",
      "          45       1.00      1.00      1.00      3401\n",
      "          46       1.00      1.00      1.00      4622\n",
      "          47       1.00      1.00      1.00      1127\n",
      "          48       1.00      1.00      1.00       112\n",
      "          49       1.00      0.99      0.99      3773\n",
      "          50       1.00      0.99      0.99      2115\n",
      "          51       0.99      1.00      1.00     15459\n",
      "          52       1.00      0.99      0.99       389\n",
      "          53       1.00      0.99      1.00      4639\n",
      "          54       1.00      0.99      1.00      1738\n",
      "          55       1.00      1.00      1.00        26\n",
      "          56       1.00      0.99      1.00       994\n",
      "          57       1.00      1.00      1.00       516\n",
      "          58       1.00      0.99      0.99      8808\n",
      "          59       1.00      0.99      1.00      1870\n",
      "          60       1.00      0.91      0.95        91\n",
      "          61       1.00      0.92      0.96        38\n",
      "          62       1.00      0.98      0.99      2292\n",
      "          63       1.00      0.74      0.85        58\n",
      "          64       1.00      0.98      0.99      4194\n",
      "          65       1.00      0.97      0.99        75\n",
      "          66       1.00      1.00      1.00        26\n",
      "          67       1.00      0.99      1.00      4310\n",
      "          68       1.00      0.99      0.99     12854\n",
      "          69       1.00      0.99      1.00       288\n",
      "          70       1.00      1.00      1.00       416\n",
      "          71       1.00      1.00      1.00        26\n",
      "          72       1.00      0.99      1.00      1351\n",
      "          73       1.00      1.00      1.00     18590\n",
      "          74       1.00      1.00      1.00       141\n",
      "          75       1.00      0.91      0.95        91\n",
      "          76       1.00      0.92      0.96        38\n",
      "          77       1.00      0.99      1.00       460\n",
      "          78       1.00      0.74      0.85        58\n",
      "          79       1.00      1.00      1.00       116\n",
      "          80       1.00      0.99      1.00      7701\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.99      1.00      1.00     13642\n",
      "          83       1.00      1.00      1.00      2099\n",
      "          84       1.00      1.00      1.00      2394\n",
      "          85       1.00      1.00      1.00     13509\n",
      "          86       1.00      1.00      1.00       835\n",
      "          87       1.00      1.00      1.00      4370\n",
      "          88       1.00      1.00      1.00       606\n",
      "          89       1.00      1.00      1.00        44\n",
      "          90       1.00      1.00      1.00        32\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    431545\n",
      "   macro avg       1.00      0.99      0.99    431545\n",
      "weighted avg       1.00      1.00      1.00    431545\n",
      " samples avg       1.00      1.00      1.00    431545\n",
      "\n",
      "================================================================================\n",
      "Validation Hamming Loss: 0.010764535244136529\n",
      "Validation Precision: 0.9628391400244116\n",
      "Validation Recall: 0.8335859288097887\n",
      "Validation F1 Score: 0.8935626347638589\n",
      "Validation Subset Accuracy: 0.5506452829757319\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     15732\n",
      "           1       0.98      0.55      0.71       209\n",
      "           2       1.00      0.43      0.60       162\n",
      "           3       1.00      0.67      0.80      1073\n",
      "           4       0.99      0.86      0.92       868\n",
      "           5       1.00      0.79      0.88       171\n",
      "           6       0.98      0.66      0.79      6859\n",
      "           7       0.98      0.69      0.81     12841\n",
      "           8       0.96      0.55      0.70      5710\n",
      "           9       1.00      0.76      0.87      1290\n",
      "          10       0.99      0.51      0.68      1880\n",
      "          11       1.00      0.70      0.82       349\n",
      "          12       0.98      0.68      0.80     12096\n",
      "          13       1.00      0.63      0.77      5294\n",
      "          14       0.99      0.62      0.76       439\n",
      "          15       1.00      0.60      0.75      4496\n",
      "          16       0.97      0.71      0.82     12415\n",
      "          17       0.99      0.56      0.71       892\n",
      "          18       0.97      0.86      0.91     19442\n",
      "          19       1.00      0.69      0.81      3644\n",
      "          20       0.95      0.97      0.96     41424\n",
      "          21       0.98      0.65      0.78      6783\n",
      "          22       1.00      0.42      0.59       315\n",
      "          23       1.00      1.00      1.00         0\n",
      "          24       0.96      0.51      0.67      5428\n",
      "          25       1.00      0.64      0.78       179\n",
      "          26       1.00      0.57      0.73       216\n",
      "          27       1.00      0.38      0.56        39\n",
      "          28       0.95      0.97      0.96     44088\n",
      "          29       1.00      0.57      0.73       699\n",
      "          30       1.00      0.54      0.70       335\n",
      "          31       1.00      0.60      0.75      4415\n",
      "          32       1.00      0.59      0.74      1186\n",
      "          33       0.99      0.55      0.71       851\n",
      "          34       1.00      0.58      0.73       574\n",
      "          35       1.00      0.74      0.85       281\n",
      "          36       0.96      0.90      0.93     22448\n",
      "          37       1.00      0.75      0.85      1523\n",
      "          38       0.98      0.79      0.88      6871\n",
      "          39       0.98      0.82      0.90     11257\n",
      "          40       0.99      0.74      0.85      2982\n",
      "          41       0.99      0.83      0.91       610\n",
      "          42       0.97      0.90      0.93     18015\n",
      "          43       0.97      0.92      0.94     13062\n",
      "          44       1.00      0.64      0.78       298\n",
      "          45       0.99      0.71      0.83      3379\n",
      "          46       1.00      0.68      0.81      4612\n",
      "          47       1.00      0.63      0.78      1218\n",
      "          48       1.00      0.58      0.74        98\n",
      "          49       0.99      0.65      0.79      3893\n",
      "          50       0.98      0.53      0.69      2146\n",
      "          51       0.94      0.96      0.95     15606\n",
      "          52       0.99      0.29      0.45       409\n",
      "          53       0.99      0.64      0.78      4727\n",
      "          54       0.99      0.56      0.71      1803\n",
      "          55       1.00      0.71      0.83        34\n",
      "          56       1.00      0.48      0.65      1058\n",
      "          57       1.00      0.71      0.83       532\n",
      "          58       0.96      0.82      0.88      8868\n",
      "          59       1.00      0.52      0.69      1847\n",
      "          60       1.00      0.57      0.72        83\n",
      "          61       1.00      0.23      0.37        40\n",
      "          62       0.98      0.57      0.72      2275\n",
      "          63       1.00      0.33      0.49        67\n",
      "          64       0.96      0.68      0.80      4200\n",
      "          65       1.00      0.43      0.61        76\n",
      "          66       1.00      0.71      0.83        34\n",
      "          67       0.99      0.64      0.78      4341\n",
      "          68       0.93      0.91      0.92     13092\n",
      "          69       1.00      0.25      0.40       317\n",
      "          70       1.00      0.54      0.70       438\n",
      "          71       1.00      0.71      0.83        34\n",
      "          72       0.99      0.49      0.66      1347\n",
      "          73       0.94      0.98      0.96     18778\n",
      "          74       1.00      0.34      0.51       130\n",
      "          75       1.00      0.57      0.72        83\n",
      "          76       1.00      0.25      0.40        40\n",
      "          77       0.99      0.39      0.56       509\n",
      "          78       1.00      0.34      0.51        67\n",
      "          79       1.00      0.50      0.66       109\n",
      "          80       0.96      0.85      0.90      8047\n",
      "          81       1.00      0.00      0.00         1\n",
      "          82       0.95      0.96      0.95     13586\n",
      "          83       1.00      0.74      0.85      2072\n",
      "          84       0.99      0.78      0.87      2291\n",
      "          85       0.97      0.95      0.96     13330\n",
      "          86       1.00      0.78      0.88       869\n",
      "          87       0.98      0.89      0.93      4547\n",
      "          88       1.00      0.71      0.83       668\n",
      "          89       1.00      0.65      0.79        51\n",
      "          90       1.00      0.33      0.50        27\n",
      "\n",
      "   micro avg       0.96      0.83      0.89    431520\n",
      "   macro avg       0.99      0.64      0.76    431520\n",
      "weighted avg       0.97      0.83      0.89    431520\n",
      " samples avg       0.95      0.84      0.87    431520\n",
      "\n",
      "================================================================================\n",
      "Testing Hamming Loss: 0.010701330569569126\n",
      "Testing Precision: 0.9631096752040705\n",
      "Testing Recall: 0.8344701682263213\n",
      "Testing F1 Score: 0.8941870322202016\n",
      "Testing Subset Accuracy: 0.5518377253814147\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     13517\n",
      "           1       1.00      0.58      0.74       189\n",
      "           2       1.00      0.49      0.66       156\n",
      "           3       1.00      0.65      0.79       982\n",
      "           4       1.00      0.87      0.93       712\n",
      "           5       1.00      0.70      0.83       175\n",
      "           6       0.98      0.65      0.78      5710\n",
      "           7       0.98      0.69      0.81     10951\n",
      "           8       0.96      0.55      0.70      5109\n",
      "           9       1.00      0.74      0.85      1013\n",
      "          10       1.00      0.54      0.70      1609\n",
      "          11       1.00      0.72      0.84       290\n",
      "          12       0.98      0.68      0.80     10479\n",
      "          13       1.00      0.62      0.76      4572\n",
      "          14       1.00      0.63      0.77       385\n",
      "          15       1.00      0.61      0.75      3910\n",
      "          16       0.97      0.71      0.82     10742\n",
      "          17       0.99      0.55      0.71       727\n",
      "          18       0.97      0.87      0.91     16873\n",
      "          19       1.00      0.69      0.82      3181\n",
      "          20       0.95      0.97      0.96     35391\n",
      "          21       0.98      0.65      0.78      5658\n",
      "          22       1.00      0.56      0.72       287\n",
      "          23       1.00      0.00      0.00         1\n",
      "          24       0.97      0.52      0.68      4870\n",
      "          25       1.00      0.67      0.80       149\n",
      "          26       1.00      0.54      0.70       176\n",
      "          27       1.00      0.26      0.41        27\n",
      "          28       0.95      0.97      0.96     37933\n",
      "          29       1.00      0.56      0.71       604\n",
      "          30       0.99      0.55      0.71       283\n",
      "          31       1.00      0.61      0.76      3837\n",
      "          32       0.99      0.57      0.73       941\n",
      "          33       0.97      0.55      0.70       694\n",
      "          34       1.00      0.57      0.73       503\n",
      "          35       1.00      0.78      0.87       250\n",
      "          36       0.96      0.89      0.93     19005\n",
      "          37       1.00      0.77      0.87      1314\n",
      "          38       0.99      0.78      0.87      5962\n",
      "          39       0.98      0.82      0.90      9795\n",
      "          40       1.00      0.74      0.85      2516\n",
      "          41       1.00      0.84      0.91       467\n",
      "          42       0.97      0.90      0.93     15685\n",
      "          43       0.97      0.92      0.94     11009\n",
      "          44       0.99      0.57      0.72       277\n",
      "          45       0.99      0.70      0.82      2837\n",
      "          46       0.99      0.68      0.81      4041\n",
      "          47       1.00      0.62      0.76      1009\n",
      "          48       1.00      0.64      0.78        92\n",
      "          49       0.99      0.66      0.79      3329\n",
      "          50       0.98      0.54      0.70      1800\n",
      "          51       0.94      0.96      0.95     13230\n",
      "          52       1.00      0.34      0.51       298\n",
      "          53       0.99      0.63      0.77      3956\n",
      "          54       0.97      0.54      0.69      1494\n",
      "          55       1.00      0.62      0.76        34\n",
      "          56       1.00      0.46      0.63       899\n",
      "          57       1.00      0.71      0.83       453\n",
      "          58       0.96      0.83      0.89      7569\n",
      "          59       1.00      0.54      0.70      1634\n",
      "          60       1.00      0.52      0.69        71\n",
      "          61       1.00      0.26      0.41        31\n",
      "          62       0.98      0.59      0.74      1956\n",
      "          63       1.00      0.38      0.55        56\n",
      "          64       0.96      0.68      0.80      3561\n",
      "          65       1.00      0.40      0.57        63\n",
      "          66       1.00      0.62      0.76        34\n",
      "          67       0.99      0.65      0.79      3755\n",
      "          68       0.93      0.91      0.92     11038\n",
      "          69       1.00      0.28      0.43       224\n",
      "          70       1.00      0.60      0.75       362\n",
      "          71       1.00      0.62      0.76        34\n",
      "          72       1.00      0.48      0.65      1119\n",
      "          73       0.95      0.98      0.96     16010\n",
      "          74       1.00      0.31      0.47       127\n",
      "          75       1.00      0.52      0.69        71\n",
      "          76       1.00      0.23      0.38        30\n",
      "          77       1.00      0.38      0.55       418\n",
      "          78       1.00      0.39      0.56        56\n",
      "          79       1.00      0.51      0.67        93\n",
      "          80       0.96      0.85      0.90      6722\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.95      0.96      0.95     11636\n",
      "          83       0.99      0.74      0.85      1809\n",
      "          84       0.98      0.80      0.88      2092\n",
      "          85       0.97      0.95      0.96     11517\n",
      "          86       1.00      0.78      0.88       698\n",
      "          87       0.98      0.89      0.93      3856\n",
      "          88       1.00      0.73      0.85       621\n",
      "          89       1.00      0.85      0.92        52\n",
      "          90       1.00      0.41      0.58        37\n",
      "\n",
      "   micro avg       0.96      0.83      0.89    369740\n",
      "   macro avg       0.99      0.65      0.76    369740\n",
      "weighted avg       0.97      0.83      0.89    369740\n",
      " samples avg       0.95      0.84      0.87    369740\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, hamming_loss, precision_score, recall_score, \n",
    "    f1_score, classification_report\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\Muralish\\Desktop\\GemAppraisal-DSGP\\notebook\\Notebook-Norman\\Datset used\\final_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.iloc[:, 92:110].values  # Features\n",
    "y = df.iloc[:, 1:92].values  # Targets (binary multi-label data)\n",
    "\n",
    "# Fill NaN values in target columns with 0\n",
    "y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "# First split: Separate test set (30%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: Separate train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize MultiOutputClassifier with RandomForest \n",
    "multi_output_model = MultiOutputClassifier(\n",
    "    estimator=RandomForestClassifier(random_state=42), n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(X, y, dataset_name):\n",
    "    y_pred = multi_output_model.predict(X)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    hamming = hamming_loss(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='micro', zero_division=1)\n",
    "    recall = recall_score(y, y_pred, average='micro', zero_division=1)\n",
    "    f1 = f1_score(y, y_pred, average='micro', zero_division=1)\n",
    "    subset_accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y, y_pred, zero_division=1)\n",
    "    \n",
    "    print(f\"{dataset_name} Hamming Loss: {hamming}\")\n",
    "    print(f\"{dataset_name} Precision: {precision}\")\n",
    "    print(f\"{dataset_name} Recall: {recall}\")\n",
    "    print(f\"{dataset_name} F1 Score: {f1}\")\n",
    "    print(f\"{dataset_name} Subset Accuracy: {subset_accuracy}\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "evaluate_model(X_train, y_train, \"Training\")\n",
    "evaluate_model(X_val, y_val, \"Validation\")\n",
    "evaluate_model(X_test, y_test, \"Testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b297d63a-e0fb-4ee8-9135-bf9e78272dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muralish\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\multiclass.py:90: UserWarning: Label not 23 is present in all training examples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Muralish\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\multiclass.py:90: UserWarning: Label not 81 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hamming Loss: 0.0001336553959409208\n",
      "Training Precision: 0.9993921184209\n",
      "Training Recall: 0.9981415611349918\n",
      "Training F1 Score: 0.9987664483218364\n",
      "Training Subset Accuracy: 0.9968107360455413\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     16068\n",
      "           1       1.00      1.00      1.00       201\n",
      "           2       1.00      0.99      1.00       193\n",
      "           3       1.00      1.00      1.00      1081\n",
      "           4       1.00      1.00      1.00       866\n",
      "           5       1.00      0.99      1.00       186\n",
      "           6       1.00      1.00      1.00      6670\n",
      "           7       1.00      1.00      1.00     12940\n",
      "           8       1.00      1.00      1.00      5940\n",
      "           9       1.00      1.00      1.00      1233\n",
      "          10       1.00      1.00      1.00      1882\n",
      "          11       1.00      1.00      1.00       367\n",
      "          12       1.00      1.00      1.00     11988\n",
      "          13       1.00      1.00      1.00      5286\n",
      "          14       1.00      1.00      1.00       464\n",
      "          15       1.00      1.00      1.00      4516\n",
      "          16       1.00      1.00      1.00     12677\n",
      "          17       1.00      1.00      1.00       887\n",
      "          18       1.00      1.00      1.00     19914\n",
      "          19       1.00      1.00      1.00      3630\n",
      "          20       1.00      1.00      1.00     41262\n",
      "          21       1.00      1.00      1.00      6601\n",
      "          22       1.00      1.00      1.00       320\n",
      "          23       1.00      1.00      1.00         0\n",
      "          24       1.00      1.00      1.00      5656\n",
      "          25       1.00      1.00      1.00       192\n",
      "          26       1.00      1.00      1.00       206\n",
      "          27       1.00      1.00      1.00        43\n",
      "          28       1.00      1.00      1.00     44284\n",
      "          29       1.00      1.00      1.00       763\n",
      "          30       1.00      1.00      1.00       338\n",
      "          31       1.00      1.00      1.00      4429\n",
      "          32       1.00      1.00      1.00      1137\n",
      "          33       1.00      1.00      1.00       849\n",
      "          34       1.00      1.00      1.00       617\n",
      "          35       1.00      1.00      1.00       304\n",
      "          36       1.00      1.00      1.00     22151\n",
      "          37       1.00      1.00      1.00      1503\n",
      "          38       1.00      1.00      1.00      6837\n",
      "          39       1.00      1.00      1.00     11656\n",
      "          40       1.00      1.00      1.00      2927\n",
      "          41       1.00      1.00      1.00       576\n",
      "          42       1.00      1.00      1.00     18279\n",
      "          43       1.00      1.00      1.00     12939\n",
      "          44       1.00      1.00      1.00       353\n",
      "          45       1.00      1.00      1.00      3401\n",
      "          46       1.00      1.00      1.00      4622\n",
      "          47       1.00      1.00      1.00      1127\n",
      "          48       1.00      1.00      1.00       112\n",
      "          49       1.00      0.99      0.99      3773\n",
      "          50       1.00      0.99      0.99      2115\n",
      "          51       0.99      1.00      1.00     15459\n",
      "          52       1.00      0.99      0.99       389\n",
      "          53       1.00      0.99      1.00      4639\n",
      "          54       1.00      0.99      1.00      1738\n",
      "          55       1.00      1.00      1.00        26\n",
      "          56       1.00      0.99      1.00       994\n",
      "          57       1.00      1.00      1.00       516\n",
      "          58       1.00      0.99      0.99      8808\n",
      "          59       1.00      0.99      1.00      1870\n",
      "          60       1.00      0.91      0.95        91\n",
      "          61       1.00      0.92      0.96        38\n",
      "          62       1.00      0.98      0.99      2292\n",
      "          63       1.00      0.74      0.85        58\n",
      "          64       1.00      0.98      0.99      4194\n",
      "          65       1.00      0.97      0.99        75\n",
      "          66       1.00      1.00      1.00        26\n",
      "          67       1.00      0.99      1.00      4310\n",
      "          68       1.00      0.99      0.99     12854\n",
      "          69       1.00      0.99      1.00       288\n",
      "          70       1.00      1.00      1.00       416\n",
      "          71       1.00      1.00      1.00        26\n",
      "          72       1.00      0.99      1.00      1351\n",
      "          73       1.00      1.00      1.00     18590\n",
      "          74       1.00      1.00      1.00       141\n",
      "          75       1.00      0.91      0.95        91\n",
      "          76       1.00      0.92      0.96        38\n",
      "          77       1.00      0.99      1.00       460\n",
      "          78       1.00      0.74      0.85        58\n",
      "          79       1.00      1.00      1.00       116\n",
      "          80       1.00      0.99      1.00      7701\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.99      1.00      1.00     13642\n",
      "          83       1.00      1.00      1.00      2099\n",
      "          84       1.00      1.00      1.00      2394\n",
      "          85       1.00      1.00      1.00     13509\n",
      "          86       1.00      1.00      1.00       835\n",
      "          87       1.00      1.00      1.00      4370\n",
      "          88       1.00      1.00      1.00       606\n",
      "          89       1.00      1.00      1.00        44\n",
      "          90       1.00      1.00      1.00        32\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    431545\n",
      "   macro avg       1.00      0.99      0.99    431545\n",
      "weighted avg       1.00      1.00      1.00    431545\n",
      " samples avg       1.00      1.00      1.00    431545\n",
      "\n",
      "================================================================================\n",
      "Validation Hamming Loss: 0.010764535244136529\n",
      "Validation Precision: 0.9628391400244116\n",
      "Validation Recall: 0.8335859288097887\n",
      "Validation F1 Score: 0.8935626347638589\n",
      "Validation Subset Accuracy: 0.5506452829757319\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     15732\n",
      "           1       0.98      0.55      0.71       209\n",
      "           2       1.00      0.43      0.60       162\n",
      "           3       1.00      0.67      0.80      1073\n",
      "           4       0.99      0.86      0.92       868\n",
      "           5       1.00      0.79      0.88       171\n",
      "           6       0.98      0.66      0.79      6859\n",
      "           7       0.98      0.69      0.81     12841\n",
      "           8       0.96      0.55      0.70      5710\n",
      "           9       1.00      0.76      0.87      1290\n",
      "          10       0.99      0.51      0.68      1880\n",
      "          11       1.00      0.70      0.82       349\n",
      "          12       0.98      0.68      0.80     12096\n",
      "          13       1.00      0.63      0.77      5294\n",
      "          14       0.99      0.62      0.76       439\n",
      "          15       1.00      0.60      0.75      4496\n",
      "          16       0.97      0.71      0.82     12415\n",
      "          17       0.99      0.56      0.71       892\n",
      "          18       0.97      0.86      0.91     19442\n",
      "          19       1.00      0.69      0.81      3644\n",
      "          20       0.95      0.97      0.96     41424\n",
      "          21       0.98      0.65      0.78      6783\n",
      "          22       1.00      0.42      0.59       315\n",
      "          23       1.00      1.00      1.00         0\n",
      "          24       0.96      0.51      0.67      5428\n",
      "          25       1.00      0.64      0.78       179\n",
      "          26       1.00      0.57      0.73       216\n",
      "          27       1.00      0.38      0.56        39\n",
      "          28       0.95      0.97      0.96     44088\n",
      "          29       1.00      0.57      0.73       699\n",
      "          30       1.00      0.54      0.70       335\n",
      "          31       1.00      0.60      0.75      4415\n",
      "          32       1.00      0.59      0.74      1186\n",
      "          33       0.99      0.55      0.71       851\n",
      "          34       1.00      0.58      0.73       574\n",
      "          35       1.00      0.74      0.85       281\n",
      "          36       0.96      0.90      0.93     22448\n",
      "          37       1.00      0.75      0.85      1523\n",
      "          38       0.98      0.79      0.88      6871\n",
      "          39       0.98      0.82      0.90     11257\n",
      "          40       0.99      0.74      0.85      2982\n",
      "          41       0.99      0.83      0.91       610\n",
      "          42       0.97      0.90      0.93     18015\n",
      "          43       0.97      0.92      0.94     13062\n",
      "          44       1.00      0.64      0.78       298\n",
      "          45       0.99      0.71      0.83      3379\n",
      "          46       1.00      0.68      0.81      4612\n",
      "          47       1.00      0.63      0.78      1218\n",
      "          48       1.00      0.58      0.74        98\n",
      "          49       0.99      0.65      0.79      3893\n",
      "          50       0.98      0.53      0.69      2146\n",
      "          51       0.94      0.96      0.95     15606\n",
      "          52       0.99      0.29      0.45       409\n",
      "          53       0.99      0.64      0.78      4727\n",
      "          54       0.99      0.56      0.71      1803\n",
      "          55       1.00      0.71      0.83        34\n",
      "          56       1.00      0.48      0.65      1058\n",
      "          57       1.00      0.71      0.83       532\n",
      "          58       0.96      0.82      0.88      8868\n",
      "          59       1.00      0.52      0.69      1847\n",
      "          60       1.00      0.57      0.72        83\n",
      "          61       1.00      0.23      0.37        40\n",
      "          62       0.98      0.57      0.72      2275\n",
      "          63       1.00      0.33      0.49        67\n",
      "          64       0.96      0.68      0.80      4200\n",
      "          65       1.00      0.43      0.61        76\n",
      "          66       1.00      0.71      0.83        34\n",
      "          67       0.99      0.64      0.78      4341\n",
      "          68       0.93      0.91      0.92     13092\n",
      "          69       1.00      0.25      0.40       317\n",
      "          70       1.00      0.54      0.70       438\n",
      "          71       1.00      0.71      0.83        34\n",
      "          72       0.99      0.49      0.66      1347\n",
      "          73       0.94      0.98      0.96     18778\n",
      "          74       1.00      0.34      0.51       130\n",
      "          75       1.00      0.57      0.72        83\n",
      "          76       1.00      0.25      0.40        40\n",
      "          77       0.99      0.39      0.56       509\n",
      "          78       1.00      0.34      0.51        67\n",
      "          79       1.00      0.50      0.66       109\n",
      "          80       0.96      0.85      0.90      8047\n",
      "          81       1.00      0.00      0.00         1\n",
      "          82       0.95      0.96      0.95     13586\n",
      "          83       1.00      0.74      0.85      2072\n",
      "          84       0.99      0.78      0.87      2291\n",
      "          85       0.97      0.95      0.96     13330\n",
      "          86       1.00      0.78      0.88       869\n",
      "          87       0.98      0.89      0.93      4547\n",
      "          88       1.00      0.71      0.83       668\n",
      "          89       1.00      0.65      0.79        51\n",
      "          90       1.00      0.33      0.50        27\n",
      "\n",
      "   micro avg       0.96      0.83      0.89    431520\n",
      "   macro avg       0.99      0.64      0.76    431520\n",
      "weighted avg       0.97      0.83      0.89    431520\n",
      " samples avg       0.95      0.84      0.87    431520\n",
      "\n",
      "================================================================================\n",
      "Testing Hamming Loss: 0.010701330569569126\n",
      "Testing Precision: 0.9631096752040705\n",
      "Testing Recall: 0.8344701682263213\n",
      "Testing F1 Score: 0.8941870322202016\n",
      "Testing Subset Accuracy: 0.5518377253814147\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     13517\n",
      "           1       1.00      0.58      0.74       189\n",
      "           2       1.00      0.49      0.66       156\n",
      "           3       1.00      0.65      0.79       982\n",
      "           4       1.00      0.87      0.93       712\n",
      "           5       1.00      0.70      0.83       175\n",
      "           6       0.98      0.65      0.78      5710\n",
      "           7       0.98      0.69      0.81     10951\n",
      "           8       0.96      0.55      0.70      5109\n",
      "           9       1.00      0.74      0.85      1013\n",
      "          10       1.00      0.54      0.70      1609\n",
      "          11       1.00      0.72      0.84       290\n",
      "          12       0.98      0.68      0.80     10479\n",
      "          13       1.00      0.62      0.76      4572\n",
      "          14       1.00      0.63      0.77       385\n",
      "          15       1.00      0.61      0.75      3910\n",
      "          16       0.97      0.71      0.82     10742\n",
      "          17       0.99      0.55      0.71       727\n",
      "          18       0.97      0.87      0.91     16873\n",
      "          19       1.00      0.69      0.82      3181\n",
      "          20       0.95      0.97      0.96     35391\n",
      "          21       0.98      0.65      0.78      5658\n",
      "          22       1.00      0.56      0.72       287\n",
      "          23       1.00      0.00      0.00         1\n",
      "          24       0.97      0.52      0.68      4870\n",
      "          25       1.00      0.67      0.80       149\n",
      "          26       1.00      0.54      0.70       176\n",
      "          27       1.00      0.26      0.41        27\n",
      "          28       0.95      0.97      0.96     37933\n",
      "          29       1.00      0.56      0.71       604\n",
      "          30       0.99      0.55      0.71       283\n",
      "          31       1.00      0.61      0.76      3837\n",
      "          32       0.99      0.57      0.73       941\n",
      "          33       0.97      0.55      0.70       694\n",
      "          34       1.00      0.57      0.73       503\n",
      "          35       1.00      0.78      0.87       250\n",
      "          36       0.96      0.89      0.93     19005\n",
      "          37       1.00      0.77      0.87      1314\n",
      "          38       0.99      0.78      0.87      5962\n",
      "          39       0.98      0.82      0.90      9795\n",
      "          40       1.00      0.74      0.85      2516\n",
      "          41       1.00      0.84      0.91       467\n",
      "          42       0.97      0.90      0.93     15685\n",
      "          43       0.97      0.92      0.94     11009\n",
      "          44       0.99      0.57      0.72       277\n",
      "          45       0.99      0.70      0.82      2837\n",
      "          46       0.99      0.68      0.81      4041\n",
      "          47       1.00      0.62      0.76      1009\n",
      "          48       1.00      0.64      0.78        92\n",
      "          49       0.99      0.66      0.79      3329\n",
      "          50       0.98      0.54      0.70      1800\n",
      "          51       0.94      0.96      0.95     13230\n",
      "          52       1.00      0.34      0.51       298\n",
      "          53       0.99      0.63      0.77      3956\n",
      "          54       0.97      0.54      0.69      1494\n",
      "          55       1.00      0.62      0.76        34\n",
      "          56       1.00      0.46      0.63       899\n",
      "          57       1.00      0.71      0.83       453\n",
      "          58       0.96      0.83      0.89      7569\n",
      "          59       1.00      0.54      0.70      1634\n",
      "          60       1.00      0.52      0.69        71\n",
      "          61       1.00      0.26      0.41        31\n",
      "          62       0.98      0.59      0.74      1956\n",
      "          63       1.00      0.38      0.55        56\n",
      "          64       0.96      0.68      0.80      3561\n",
      "          65       1.00      0.40      0.57        63\n",
      "          66       1.00      0.62      0.76        34\n",
      "          67       0.99      0.65      0.79      3755\n",
      "          68       0.93      0.91      0.92     11038\n",
      "          69       1.00      0.28      0.43       224\n",
      "          70       1.00      0.60      0.75       362\n",
      "          71       1.00      0.62      0.76        34\n",
      "          72       1.00      0.48      0.65      1119\n",
      "          73       0.95      0.98      0.96     16010\n",
      "          74       1.00      0.31      0.47       127\n",
      "          75       1.00      0.52      0.69        71\n",
      "          76       1.00      0.23      0.38        30\n",
      "          77       1.00      0.38      0.55       418\n",
      "          78       1.00      0.39      0.56        56\n",
      "          79       1.00      0.51      0.67        93\n",
      "          80       0.96      0.85      0.90      6722\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.95      0.96      0.95     11636\n",
      "          83       0.99      0.74      0.85      1809\n",
      "          84       0.98      0.80      0.88      2092\n",
      "          85       0.97      0.95      0.96     11517\n",
      "          86       1.00      0.78      0.88       698\n",
      "          87       0.98      0.89      0.93      3856\n",
      "          88       1.00      0.73      0.85       621\n",
      "          89       1.00      0.85      0.92        52\n",
      "          90       1.00      0.41      0.58        37\n",
      "\n",
      "   micro avg       0.96      0.83      0.89    369740\n",
      "   macro avg       0.99      0.65      0.76    369740\n",
      "weighted avg       0.97      0.83      0.89    369740\n",
      " samples avg       0.95      0.84      0.87    369740\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, hamming_loss, precision_score, recall_score, \n",
    "    f1_score, classification_report\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\Muralish\\Desktop\\GemAppraisal-DSGP\\notebook\\Notebook-Norman\\Datset used\\final_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.iloc[:, 92:110].values  # Features\n",
    "y = df.iloc[:, 1:92].values  # Targets (binary multi-label data)\n",
    "\n",
    "# Fill NaN values in target columns with 0\n",
    "y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "# First split: Separate test set (30%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: Separate train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize OneVsRestClassifier with RandomForest\n",
    "ovr_model = OneVsRestClassifier(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ovr_model.fit(X_train, y_train)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(X, y, dataset_name):\n",
    "    y_pred = ovr_model.predict(X)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    hamming = hamming_loss(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='micro', zero_division=1)\n",
    "    recall = recall_score(y, y_pred, average='micro', zero_division=1)\n",
    "    f1 = f1_score(y, y_pred, average='micro', zero_division=1)\n",
    "    subset_accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y, y_pred, zero_division=1)\n",
    "    \n",
    "    print(f\"{dataset_name} Hamming Loss: {hamming}\")\n",
    "    print(f\"{dataset_name} Precision: {precision}\")\n",
    "    print(f\"{dataset_name} Recall: {recall}\")\n",
    "    print(f\"{dataset_name} F1 Score: {f1}\")\n",
    "    print(f\"{dataset_name} Subset Accuracy: {subset_accuracy}\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "evaluate_model(X_train, y_train, \"Training\")\n",
    "evaluate_model(X_val, y_val, \"Validation\")\n",
    "evaluate_model(X_test, y_test, \"Testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6c069a-b007-4454-880e-c722a100d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hamming Loss: 0.0001336553959409208\n",
      "Training Precision: 0.9993921184209\n",
      "Training Recall: 0.9981415611349918\n",
      "Training F1 Score: 0.9987664483218364\n",
      "Training Subset Accuracy: 0.9968107360455413\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     16068\n",
      "           1       1.00      1.00      1.00       201\n",
      "           2       1.00      0.99      1.00       193\n",
      "           3       1.00      1.00      1.00      1081\n",
      "           4       1.00      1.00      1.00       866\n",
      "           5       1.00      0.99      1.00       186\n",
      "           6       1.00      1.00      1.00      6670\n",
      "           7       1.00      1.00      1.00     12940\n",
      "           8       1.00      1.00      1.00      5940\n",
      "           9       1.00      1.00      1.00      1233\n",
      "          10       1.00      1.00      1.00      1882\n",
      "          11       1.00      1.00      1.00       367\n",
      "          12       1.00      1.00      1.00     11988\n",
      "          13       1.00      1.00      1.00      5286\n",
      "          14       1.00      1.00      1.00       464\n",
      "          15       1.00      1.00      1.00      4516\n",
      "          16       1.00      1.00      1.00     12677\n",
      "          17       1.00      1.00      1.00       887\n",
      "          18       1.00      1.00      1.00     19914\n",
      "          19       1.00      1.00      1.00      3630\n",
      "          20       1.00      1.00      1.00     41262\n",
      "          21       1.00      1.00      1.00      6601\n",
      "          22       1.00      1.00      1.00       320\n",
      "          23       1.00      1.00      1.00         0\n",
      "          24       1.00      1.00      1.00      5656\n",
      "          25       1.00      1.00      1.00       192\n",
      "          26       1.00      1.00      1.00       206\n",
      "          27       1.00      1.00      1.00        43\n",
      "          28       1.00      1.00      1.00     44284\n",
      "          29       1.00      1.00      1.00       763\n",
      "          30       1.00      1.00      1.00       338\n",
      "          31       1.00      1.00      1.00      4429\n",
      "          32       1.00      1.00      1.00      1137\n",
      "          33       1.00      1.00      1.00       849\n",
      "          34       1.00      1.00      1.00       617\n",
      "          35       1.00      1.00      1.00       304\n",
      "          36       1.00      1.00      1.00     22151\n",
      "          37       1.00      1.00      1.00      1503\n",
      "          38       1.00      1.00      1.00      6837\n",
      "          39       1.00      1.00      1.00     11656\n",
      "          40       1.00      1.00      1.00      2927\n",
      "          41       1.00      1.00      1.00       576\n",
      "          42       1.00      1.00      1.00     18279\n",
      "          43       1.00      1.00      1.00     12939\n",
      "          44       1.00      1.00      1.00       353\n",
      "          45       1.00      1.00      1.00      3401\n",
      "          46       1.00      1.00      1.00      4622\n",
      "          47       1.00      1.00      1.00      1127\n",
      "          48       1.00      1.00      1.00       112\n",
      "          49       1.00      0.99      0.99      3773\n",
      "          50       1.00      0.99      0.99      2115\n",
      "          51       0.99      1.00      1.00     15459\n",
      "          52       1.00      0.99      0.99       389\n",
      "          53       1.00      0.99      1.00      4639\n",
      "          54       1.00      0.99      1.00      1738\n",
      "          55       1.00      1.00      1.00        26\n",
      "          56       1.00      0.99      1.00       994\n",
      "          57       1.00      1.00      1.00       516\n",
      "          58       1.00      0.99      0.99      8808\n",
      "          59       1.00      0.99      1.00      1870\n",
      "          60       1.00      0.91      0.95        91\n",
      "          61       1.00      0.92      0.96        38\n",
      "          62       1.00      0.98      0.99      2292\n",
      "          63       1.00      0.74      0.85        58\n",
      "          64       1.00      0.98      0.99      4194\n",
      "          65       1.00      0.97      0.99        75\n",
      "          66       1.00      1.00      1.00        26\n",
      "          67       1.00      0.99      1.00      4310\n",
      "          68       1.00      0.99      0.99     12854\n",
      "          69       1.00      0.99      1.00       288\n",
      "          70       1.00      1.00      1.00       416\n",
      "          71       1.00      1.00      1.00        26\n",
      "          72       1.00      0.99      1.00      1351\n",
      "          73       1.00      1.00      1.00     18590\n",
      "          74       1.00      1.00      1.00       141\n",
      "          75       1.00      0.91      0.95        91\n",
      "          76       1.00      0.92      0.96        38\n",
      "          77       1.00      0.99      1.00       460\n",
      "          78       1.00      0.74      0.85        58\n",
      "          79       1.00      1.00      1.00       116\n",
      "          80       1.00      0.99      1.00      7701\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.99      1.00      1.00     13642\n",
      "          83       1.00      1.00      1.00      2099\n",
      "          84       1.00      1.00      1.00      2394\n",
      "          85       1.00      1.00      1.00     13509\n",
      "          86       1.00      1.00      1.00       835\n",
      "          87       1.00      1.00      1.00      4370\n",
      "          88       1.00      1.00      1.00       606\n",
      "          89       1.00      1.00      1.00        44\n",
      "          90       1.00      1.00      1.00        32\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    431545\n",
      "   macro avg       1.00      0.99      0.99    431545\n",
      "weighted avg       1.00      1.00      1.00    431545\n",
      " samples avg       1.00      1.00      1.00    431545\n",
      "\n",
      "================================================================================\n",
      "Validation Hamming Loss: 0.010764535244136529\n",
      "Validation Precision: 0.9628391400244116\n",
      "Validation Recall: 0.8335859288097887\n",
      "Validation F1 Score: 0.8935626347638589\n",
      "Validation Subset Accuracy: 0.5506452829757319\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     15732\n",
      "           1       0.98      0.55      0.71       209\n",
      "           2       1.00      0.43      0.60       162\n",
      "           3       1.00      0.67      0.80      1073\n",
      "           4       0.99      0.86      0.92       868\n",
      "           5       1.00      0.79      0.88       171\n",
      "           6       0.98      0.66      0.79      6859\n",
      "           7       0.98      0.69      0.81     12841\n",
      "           8       0.96      0.55      0.70      5710\n",
      "           9       1.00      0.76      0.87      1290\n",
      "          10       0.99      0.51      0.68      1880\n",
      "          11       1.00      0.70      0.82       349\n",
      "          12       0.98      0.68      0.80     12096\n",
      "          13       1.00      0.63      0.77      5294\n",
      "          14       0.99      0.62      0.76       439\n",
      "          15       1.00      0.60      0.75      4496\n",
      "          16       0.97      0.71      0.82     12415\n",
      "          17       0.99      0.56      0.71       892\n",
      "          18       0.97      0.86      0.91     19442\n",
      "          19       1.00      0.69      0.81      3644\n",
      "          20       0.95      0.97      0.96     41424\n",
      "          21       0.98      0.65      0.78      6783\n",
      "          22       1.00      0.42      0.59       315\n",
      "          23       1.00      1.00      1.00         0\n",
      "          24       0.96      0.51      0.67      5428\n",
      "          25       1.00      0.64      0.78       179\n",
      "          26       1.00      0.57      0.73       216\n",
      "          27       1.00      0.38      0.56        39\n",
      "          28       0.95      0.97      0.96     44088\n",
      "          29       1.00      0.57      0.73       699\n",
      "          30       1.00      0.54      0.70       335\n",
      "          31       1.00      0.60      0.75      4415\n",
      "          32       1.00      0.59      0.74      1186\n",
      "          33       0.99      0.55      0.71       851\n",
      "          34       1.00      0.58      0.73       574\n",
      "          35       1.00      0.74      0.85       281\n",
      "          36       0.96      0.90      0.93     22448\n",
      "          37       1.00      0.75      0.85      1523\n",
      "          38       0.98      0.79      0.88      6871\n",
      "          39       0.98      0.82      0.90     11257\n",
      "          40       0.99      0.74      0.85      2982\n",
      "          41       0.99      0.83      0.91       610\n",
      "          42       0.97      0.90      0.93     18015\n",
      "          43       0.97      0.92      0.94     13062\n",
      "          44       1.00      0.64      0.78       298\n",
      "          45       0.99      0.71      0.83      3379\n",
      "          46       1.00      0.68      0.81      4612\n",
      "          47       1.00      0.63      0.78      1218\n",
      "          48       1.00      0.58      0.74        98\n",
      "          49       0.99      0.65      0.79      3893\n",
      "          50       0.98      0.53      0.69      2146\n",
      "          51       0.94      0.96      0.95     15606\n",
      "          52       0.99      0.29      0.45       409\n",
      "          53       0.99      0.64      0.78      4727\n",
      "          54       0.99      0.56      0.71      1803\n",
      "          55       1.00      0.71      0.83        34\n",
      "          56       1.00      0.48      0.65      1058\n",
      "          57       1.00      0.71      0.83       532\n",
      "          58       0.96      0.82      0.88      8868\n",
      "          59       1.00      0.52      0.69      1847\n",
      "          60       1.00      0.57      0.72        83\n",
      "          61       1.00      0.23      0.37        40\n",
      "          62       0.98      0.57      0.72      2275\n",
      "          63       1.00      0.33      0.49        67\n",
      "          64       0.96      0.68      0.80      4200\n",
      "          65       1.00      0.43      0.61        76\n",
      "          66       1.00      0.71      0.83        34\n",
      "          67       0.99      0.64      0.78      4341\n",
      "          68       0.93      0.91      0.92     13092\n",
      "          69       1.00      0.25      0.40       317\n",
      "          70       1.00      0.54      0.70       438\n",
      "          71       1.00      0.71      0.83        34\n",
      "          72       0.99      0.49      0.66      1347\n",
      "          73       0.94      0.98      0.96     18778\n",
      "          74       1.00      0.34      0.51       130\n",
      "          75       1.00      0.57      0.72        83\n",
      "          76       1.00      0.25      0.40        40\n",
      "          77       0.99      0.39      0.56       509\n",
      "          78       1.00      0.34      0.51        67\n",
      "          79       1.00      0.50      0.66       109\n",
      "          80       0.96      0.85      0.90      8047\n",
      "          81       1.00      0.00      0.00         1\n",
      "          82       0.95      0.96      0.95     13586\n",
      "          83       1.00      0.74      0.85      2072\n",
      "          84       0.99      0.78      0.87      2291\n",
      "          85       0.97      0.95      0.96     13330\n",
      "          86       1.00      0.78      0.88       869\n",
      "          87       0.98      0.89      0.93      4547\n",
      "          88       1.00      0.71      0.83       668\n",
      "          89       1.00      0.65      0.79        51\n",
      "          90       1.00      0.33      0.50        27\n",
      "\n",
      "   micro avg       0.96      0.83      0.89    431520\n",
      "   macro avg       0.99      0.64      0.76    431520\n",
      "weighted avg       0.97      0.83      0.89    431520\n",
      " samples avg       0.95      0.84      0.87    431520\n",
      "\n",
      "================================================================================\n",
      "Testing Hamming Loss: 0.010701330569569126\n",
      "Testing Precision: 0.9631096752040705\n",
      "Testing Recall: 0.8344701682263213\n",
      "Testing F1 Score: 0.8941870322202016\n",
      "Testing Subset Accuracy: 0.5518377253814147\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     13517\n",
      "           1       1.00      0.58      0.74       189\n",
      "           2       1.00      0.49      0.66       156\n",
      "           3       1.00      0.65      0.79       982\n",
      "           4       1.00      0.87      0.93       712\n",
      "           5       1.00      0.70      0.83       175\n",
      "           6       0.98      0.65      0.78      5710\n",
      "           7       0.98      0.69      0.81     10951\n",
      "           8       0.96      0.55      0.70      5109\n",
      "           9       1.00      0.74      0.85      1013\n",
      "          10       1.00      0.54      0.70      1609\n",
      "          11       1.00      0.72      0.84       290\n",
      "          12       0.98      0.68      0.80     10479\n",
      "          13       1.00      0.62      0.76      4572\n",
      "          14       1.00      0.63      0.77       385\n",
      "          15       1.00      0.61      0.75      3910\n",
      "          16       0.97      0.71      0.82     10742\n",
      "          17       0.99      0.55      0.71       727\n",
      "          18       0.97      0.87      0.91     16873\n",
      "          19       1.00      0.69      0.82      3181\n",
      "          20       0.95      0.97      0.96     35391\n",
      "          21       0.98      0.65      0.78      5658\n",
      "          22       1.00      0.56      0.72       287\n",
      "          23       1.00      0.00      0.00         1\n",
      "          24       0.97      0.52      0.68      4870\n",
      "          25       1.00      0.67      0.80       149\n",
      "          26       1.00      0.54      0.70       176\n",
      "          27       1.00      0.26      0.41        27\n",
      "          28       0.95      0.97      0.96     37933\n",
      "          29       1.00      0.56      0.71       604\n",
      "          30       0.99      0.55      0.71       283\n",
      "          31       1.00      0.61      0.76      3837\n",
      "          32       0.99      0.57      0.73       941\n",
      "          33       0.97      0.55      0.70       694\n",
      "          34       1.00      0.57      0.73       503\n",
      "          35       1.00      0.78      0.87       250\n",
      "          36       0.96      0.89      0.93     19005\n",
      "          37       1.00      0.77      0.87      1314\n",
      "          38       0.99      0.78      0.87      5962\n",
      "          39       0.98      0.82      0.90      9795\n",
      "          40       1.00      0.74      0.85      2516\n",
      "          41       1.00      0.84      0.91       467\n",
      "          42       0.97      0.90      0.93     15685\n",
      "          43       0.97      0.92      0.94     11009\n",
      "          44       0.99      0.57      0.72       277\n",
      "          45       0.99      0.70      0.82      2837\n",
      "          46       0.99      0.68      0.81      4041\n",
      "          47       1.00      0.62      0.76      1009\n",
      "          48       1.00      0.64      0.78        92\n",
      "          49       0.99      0.66      0.79      3329\n",
      "          50       0.98      0.54      0.70      1800\n",
      "          51       0.94      0.96      0.95     13230\n",
      "          52       1.00      0.34      0.51       298\n",
      "          53       0.99      0.63      0.77      3956\n",
      "          54       0.97      0.54      0.69      1494\n",
      "          55       1.00      0.62      0.76        34\n",
      "          56       1.00      0.46      0.63       899\n",
      "          57       1.00      0.71      0.83       453\n",
      "          58       0.96      0.83      0.89      7569\n",
      "          59       1.00      0.54      0.70      1634\n",
      "          60       1.00      0.52      0.69        71\n",
      "          61       1.00      0.26      0.41        31\n",
      "          62       0.98      0.59      0.74      1956\n",
      "          63       1.00      0.38      0.55        56\n",
      "          64       0.96      0.68      0.80      3561\n",
      "          65       1.00      0.40      0.57        63\n",
      "          66       1.00      0.62      0.76        34\n",
      "          67       0.99      0.65      0.79      3755\n",
      "          68       0.93      0.91      0.92     11038\n",
      "          69       1.00      0.28      0.43       224\n",
      "          70       1.00      0.60      0.75       362\n",
      "          71       1.00      0.62      0.76        34\n",
      "          72       1.00      0.48      0.65      1119\n",
      "          73       0.95      0.98      0.96     16010\n",
      "          74       1.00      0.31      0.47       127\n",
      "          75       1.00      0.52      0.69        71\n",
      "          76       1.00      0.23      0.38        30\n",
      "          77       1.00      0.38      0.55       418\n",
      "          78       1.00      0.39      0.56        56\n",
      "          79       1.00      0.51      0.67        93\n",
      "          80       0.96      0.85      0.90      6722\n",
      "          81       1.00      1.00      1.00         0\n",
      "          82       0.95      0.96      0.95     11636\n",
      "          83       0.99      0.74      0.85      1809\n",
      "          84       0.98      0.80      0.88      2092\n",
      "          85       0.97      0.95      0.96     11517\n",
      "          86       1.00      0.78      0.88       698\n",
      "          87       0.98      0.89      0.93      3856\n",
      "          88       1.00      0.73      0.85       621\n",
      "          89       1.00      0.85      0.92        52\n",
      "          90       1.00      0.41      0.58        37\n",
      "\n",
      "   micro avg       0.96      0.83      0.89    369740\n",
      "   macro avg       0.99      0.65      0.76    369740\n",
      "weighted avg       0.97      0.83      0.89    369740\n",
      " samples avg       0.95      0.84      0.87    369740\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, hamming_loss, precision_score, recall_score, \n",
    "    f1_score, classification_report\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\Muralish\\Desktop\\GemAppraisal-DSGP\\notebook\\Notebook-Norman\\Datset used\\final_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.iloc[:, 92:110].values  # Features\n",
    "y = df.iloc[:, 1:92].values  # Targets (binary multi-label data)\n",
    "\n",
    "# Fill NaN values in target columns with 0\n",
    "y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "# First split: Separate test set (30%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: Separate train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Compute class weights for each label\n",
    "class_weights = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    unique_classes = np.unique(y_train[:, i])\n",
    "    \n",
    "    # Skip labels with only one unique class (either all 0s or all 1s)\n",
    "    if len(unique_classes) == 1:\n",
    "        class_weights.append(None)  # No weighting for this label\n",
    "    else:\n",
    "        weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=unique_classes,\n",
    "            y=y_train[:, i]\n",
    "        )\n",
    "        class_weights.append({unique_classes[j]: weights[j] for j in range(len(unique_classes))})\n",
    "\n",
    "# Initialize MultiOutputClassifier with class-weighted RandomForest\n",
    "estimators = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    if class_weights[i] is None:\n",
    "        # No class weighting for this label\n",
    "        clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    else:\n",
    "        # Apply class weights\n",
    "        clf = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=class_weights[i])\n",
    "    \n",
    "    estimators.append(clf)\n",
    "\n",
    "# Train the model\n",
    "multi_output_model = MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(X, y, dataset_name):\n",
    "    y_pred = multi_output_model.predict(X)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    hamming = hamming_loss(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='micro', zero_division=1)\n",
    "    recall = recall_score(y, y_pred, average='micro', zero_division=1)\n",
    "    f1 = f1_score(y, y_pred, average='micro', zero_division=1)\n",
    "    subset_accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y, y_pred, zero_division=1)\n",
    "    \n",
    "    print(f\"{dataset_name} Hamming Loss: {hamming}\")\n",
    "    print(f\"{dataset_name} Precision: {precision}\")\n",
    "    print(f\"{dataset_name} Recall: {recall}\")\n",
    "    print(f\"{dataset_name} F1 Score: {f1}\")\n",
    "    print(f\"{dataset_name} Subset Accuracy: {subset_accuracy}\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "evaluate_model(X_train, y_train, \"Training\")\n",
    "evaluate_model(X_val, y_val, \"Validation\")\n",
    "evaluate_model(X_test, y_test, \"Testing\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
