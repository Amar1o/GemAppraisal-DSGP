{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb75eb0-6c91-4068-b392-090df157a9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA results saved to C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_final_result.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\combined_data.csv'  # Update with the correct file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Remove rows with NaN values (if any)\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Extract the 'ItemID' column and features for PCA (excluding 'ItemID')\n",
    "itemid = data_clean['ItemID']  # Assuming 'ItemID' is the correct column name\n",
    "features = data_clean.drop(columns=['ItemID'])  # Drop the 'ItemID' column\n",
    "\n",
    "# Step 2: Normalize the features (Standardization)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Step 3: Perform PCA (retain 95% of variance)\n",
    "pca = PCA(n_components=30)  # Retain 95% of the variance\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Convert the PCA results to a DataFrame with the original 'ItemID'\n",
    "pca_df = pd.DataFrame(features_pca)\n",
    "pca_df['ItemID'] = itemid  # Add 'ItemID' back to the DataFrame\n",
    "\n",
    "# Save the PCA results to a CSV file in the same directory as the input file\n",
    "output_file = r'C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_final_result.csv'\n",
    "pca_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"PCA results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8b5001-9b87-4443-bbe4-c12b25a8beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully merged and saved to: C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_final_combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the input files\n",
    "encoded_csv_path = r'C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_encoded.csv'\n",
    "combined_pca_path = r'C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_PCA.csv'\n",
    "\n",
    "# Path to save the final combined file\n",
    "output_path = r'C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_final_combined_data.csv'\n",
    "\n",
    "# Load the encoded CSV and combined PCA CSV into DataFrames\n",
    "encoded_df = pd.read_csv(encoded_csv_path)\n",
    "combined_pca_df = pd.read_csv(combined_pca_path)\n",
    "\n",
    "# Ensure the 'Item ID' column exists in both DataFrames\n",
    "if 'ItemID' in encoded_df.columns and 'ItemID' in combined_pca_df.columns:\n",
    "    # Merge the DataFrames on 'Item ID'\n",
    "    merged_df = pd.merge(encoded_df, combined_pca_df, on='ItemID', how='inner')  # Use 'inner' for items present in both files\n",
    "    \n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Data successfully merged and saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"Error: 'Item ID' column not found in one or both files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b62f6c3-2bd0-4704-adbc-263e0a818bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hamming Loss: 1.8406625362540218e-05\n",
      "Training Precision: 0.9998766902738871\n",
      "Training Recall: 0.9997882982435733\n",
      "Training F1 Score: 0.9998324923051153\n",
      "Training Subset Accuracy: 0.9994649296266139\n",
      "\n",
      "Validation Hamming Loss: 0.016398706387190876\n",
      "Validation Precision: 0.93006314852739\n",
      "Validation Recall: 0.7585860348256971\n",
      "Validation F1 Score: 0.8356181058021134\n",
      "Validation Subset Accuracy: 0.45972479091786767\n",
      "\n",
      "Testing Hamming Loss: 0.01640626980564557\n",
      "Testing Precision: 0.9296146928081975\n",
      "Testing Recall: 0.7588628036368571\n",
      "Testing F1 Score: 0.8356048731140442\n",
      "Testing Subset Accuracy: 0.4586511059845298\n",
      "\n",
      "Model saved at: C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\rf_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\updated_final_combined_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.iloc[:, 92:122].values  # Features\n",
    "y = df.iloc[:, 1:92].values  # Targets (binary multi-label data)\n",
    "\n",
    "# First split: Separate test set (30%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: Separate train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Compute class weights for each label\n",
    "class_weights = []\n",
    "for i in range(y.shape[1]):  # Loop through each label column\n",
    "    class_weight = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train[:, i]),\n",
    "        y=y_train[:, i]\n",
    "    )\n",
    "    class_weights.append(dict(enumerate(class_weight)))\n",
    "\n",
    "# Initialize MultiOutputClassifier with RandomForest\n",
    "multi_output_model = MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42), n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(X, y, dataset_name):\n",
    "    y_pred = multi_output_model.predict(X)\n",
    "    hamming = hamming_loss(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='micro')\n",
    "    recall = recall_score(y, y_pred, average='micro')\n",
    "    f1 = f1_score(y, y_pred, average='micro')\n",
    "    subset_accuracy = accuracy_score(y, y_pred)\n",
    "    print(f\"{dataset_name} Hamming Loss: {hamming}\")\n",
    "    print(f\"{dataset_name} Precision: {precision}\")\n",
    "    print(f\"{dataset_name} Recall: {recall}\")\n",
    "    print(f\"{dataset_name} F1 Score: {f1}\")\n",
    "    print(f\"{dataset_name} Subset Accuracy: {subset_accuracy}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "evaluate_model(X_train, y_train, \"Training\")\n",
    "evaluate_model(X_val, y_val, \"Validation\")\n",
    "evaluate_model(X_test, y_test, \"Testing\")\n",
    "\n",
    "# Save the model\n",
    "save_path = r\"C:\\Users\\Muralish\\Desktop\\Sapphires_Cleaned\\combine\\rf_model.joblib\"\n",
    "joblib.dump(multi_output_model, save_path)\n",
    "\n",
    "print(f\"Model saved at: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
